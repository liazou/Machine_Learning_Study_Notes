{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Based Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "$\\textbf{Classification}$:\n",
    "- Entropy\n",
    "$$E(p) = - \\sum_{i=1}^c p_i log_2(p_i)$$\n",
    "- Gini Impurity\n",
    "$$I_{G}(p) = \\sum_{i=1}^J p_i \\sum_{k \\neq i} p_k = \\sum_{i=1}^J p_i (1 - p_i) = 1 - \\sum_{i=1}^J p_i^2$$\n",
    "\n",
    "$\\textbf{Regression}$:\n",
    "- Variance\n",
    "$$V(S) = \\frac{1}{|S|^2}\\sum_{i \\in S} \\sum_{j \\in S} (x_i - x_j)^2$$\n",
    "\n",
    "$\\textbf{How to split node}$:\n",
    "1. Compute entropy/gini impurity of the target\n",
    "2. Dataset is split on different attributes. The entropy/gini impurity of each branch is calculated\n",
    "3. Compute information gain (difference of entropy)\n",
    "4. Choose the split with the largest information gain as the decision node\n",
    "\n",
    "$\\textbf{How to split continuous features}$:\n",
    "1. Sort values in ascent order: $a_1, a_2, ..., a_M$\n",
    "2. Choose $M-1$ splitting points: $\\frac{a_1 + a_2}{2}, \\frac{a_2 + a_3}{2}, ..., \\frac{a_{M-1} + a_M}{2}$\n",
    "3. Find the splitting point just as categorial features\n",
    "\n",
    "$\\textbf{Different types of decision tree}$: https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "- ID3: Information Gain (discrete features)\n",
    "- C.4.5: Information Gain (continuous features)\n",
    "- CART: Gini impurity\n",
    "- CHAID:\n",
    "  - Continuous response: F-test\n",
    "  - Categorical response: $\\chi^2$-test\n",
    "- MARS\n",
    "- Conditional Inference Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Regularization of Decision Tree}$: http://cs229.stanford.edu/notes/cs229-notes-dt.pdf\n",
    "- Min Leaf Size: Do not split if its cardinality falls below a fixed threshold\n",
    "- Max Depth: Do not split if more than a fixed threshold of splits were already taken to reach region\n",
    "- Max Number of Nodes: Stop if a tree has more than a fixed threshold of leaf nodes\n",
    "\n",
    "$\\textbf{Advantages}$:\n",
    "- easily interpret (white-box)\n",
    "- not sensitive to outliers\n",
    "- works well in qualitative (categorical) features\n",
    "\n",
    "$\\textbf{Disadvantages}$:\n",
    "- overfitting\n",
    "  - How to overcome overfitting\n",
    "    - Regularization\n",
    "    - Ensemble learning (bagging, boosting)\n",
    "- can only output discrete values\n",
    "- sensitive to direction of decision boundary\n",
    "  - How to resolve direction problem\n",
    "    - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class BinaryTree:\n",
    "    def __init__(self, feature_name, label, threshold):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.feature_name = feature_name\n",
    "        self.threshold = threshold\n",
    "        self.label = label\n",
    "    \n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def compute_entropy(self, p0, p1):\n",
    "        return -(p0 * np.log2(p0) + p1 * np.log2(p1))\n",
    "\n",
    "    def generate_thresholds(self, array):\n",
    "        thresholds = []\n",
    "        array = np.sort(array)\n",
    "        for i in range(len(array)-1):\n",
    "            thresholds.append((array[i] + array[i+1]) / 2)\n",
    "        return thresholds\n",
    "\n",
    "    def grow_tree(self, X, y, depth):\n",
    "        # X: pd.DataFrame\n",
    "        # y: pd.Series\n",
    "        if depth >= self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        best_gain = 0\n",
    "        best_feature = ''\n",
    "        best_threshold = None\n",
    "\n",
    "        for col in X.columns:\n",
    "            before_entropy = self.compute_entropy(\n",
    "                np.mean(y), # propotion of 0's\n",
    "                1 - np.mean(y) # propotion of 1's\n",
    "            )\n",
    "            thresholds = self.generate_thresholds(np.unique(X[col]))\n",
    "            for threshold in thresholds:\n",
    "                left_index = X[col] < threshold\n",
    "                right_index = X[col] >= threshold\n",
    "\n",
    "                left_entropy = self.compute_entropy(\n",
    "                    np.mean(y[left_index]), 1 - np.mean(y[left_index])\n",
    "                )\n",
    "                right_entropy = self.compute_entropy(\n",
    "                    np.mean(y[right_index]), 1 - np.mean(y[right_index])\n",
    "                )\n",
    "\n",
    "                print([before_entropy, left_entropy, right_entropy])\n",
    "\n",
    "                information_gain = abs(before_entropy - (left_entropy + right_entropy))\n",
    "                if information_gain > best_gain:\n",
    "                    best_gain = information_gain\n",
    "                    best_feature = col\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        predict_label = np.argmax([sum(y==i) for i in [0, 1]])\n",
    "        node = BinaryTree(best_feature, predict_label, best_threshold)\n",
    "        X_left, y_left = X[X[col]<best_threshold], y[X[col]<best_threshold]\n",
    "        X_right, y_right = X[X[col]>=best_threshold], y[X[col]>=best_threshold]\n",
    "        \n",
    "        node.left = self.grow_tree(X_left, y_left, depth+1)\n",
    "        node.right = self.grow_tree(X_right, y_right, depth+1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.grow_tree(X, y, 0)\n",
    "    \n",
    "    def predict_row(self, row):\n",
    "        node = self.tree\n",
    "        while node.left:\n",
    "            if row[node.feature_name] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.label\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.apply(\n",
    "            (lambda row: self.predict_row(row)),\n",
    "            axis=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 100\n",
    "\n",
    "X_train = pd.DataFrame({\n",
    "    'x1': np.random.randint(5, size=n_train),\n",
    "    'x2': np.random.randint(2, size=n_train),\n",
    "    'x3': np.random.randint(4, size=n_train),\n",
    "    'x4': np.random.randint(2, size=n_train),\n",
    "})\n",
    "\n",
    "y_train = pd.Series(np.random.randint(2, size=n_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9953784388202257, 0.9910760598382222, 0.9966132830150962]\n",
      "[0.9953784388202257, 0.9839393951635756, 0.999743186071085]\n",
      "[0.9953784388202257, 0.9975025463691152, 0.9886994082884974]\n",
      "[0.9953784388202257, 0.997249632970471, 0.9819407868640977]\n",
      "[0.9953784388202257, 0.9924760039430819, 0.9975025463691152]\n",
      "[0.9953784388202257, 0.9709505944546686, 0.9988455359952018]\n",
      "[0.9953784388202257, 0.9953784388202257, 0.9953784388202257]\n",
      "[0.9953784388202257, 0.9899927915575186, 1.0]\n",
      "[0.9953784388202257, 0.9990102708804813, 0.9876925088958033]\n",
      "[0.9953784388202257, 0.9910760598382222, 0.9966132830150962]\n",
      "[0.9953784388202257, 0.9839393951635756, 0.999743186071085]\n",
      "[0.9953784388202257, 0.9975025463691152, 0.9886994082884974]\n",
      "[0.9953784388202257, 0.997249632970471, 0.9819407868640977]\n",
      "[0.9953784388202257, 0.9924760039430819, 0.9975025463691152]\n",
      "[0.9953784388202257, 0.9709505944546686, 0.9988455359952018]\n",
      "[0.9953784388202257, 0.9953784388202257, 0.9953784388202257]\n",
      "[0.9953784388202257, 0.9899927915575186, 1.0]\n",
      "[0.9953784388202257, 0.9990102708804813, 0.9876925088958033]\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(max_depth=2)\n",
    "decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = 10\n",
    "\n",
    "X_val = pd.DataFrame({\n",
    "    'x1': np.random.randint(5, size=n_val),\n",
    "    'x2': np.random.randint(2, size=n_val),\n",
    "    'x3': np.random.randint(4, size=n_val),\n",
    "    'x4': np.random.randint(2, size=n_val),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "5    0\n",
       "6    0\n",
       "7    1\n",
       "8    1\n",
       "9    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "![random_forest](https://miro.medium.com/max/592/1*i0o8mjFfCn-uD79-F1Cqkw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Steps}$:\n",
    "1. Sample data (row) with replacement\n",
    "2. Sample feature (column) without replacement\n",
    "  - bagging tree does not sample feature\n",
    "3. Build every decision tree independently\n",
    "4. Final prediction: average or majority vote\n",
    "5. $\\underline{\\textbf{Note}}$: No loss function, use out-of-bag errors\n",
    "\n",
    "$\\textbf{Hyperparameters}$:\n",
    "- max_features: max # of features\n",
    "- n_estimators: number of decision trees\n",
    "- min_samples_leaf\n",
    "\n",
    "$\\textbf{Feature Importance}$: https://blog.datadive.net/selecting-good-features-part-iii-random-forests/\n",
    "1. Each split will calculate decrease of variance/entropy\n",
    "2. Sum up total decrease of variance/entropy by feature\n",
    "3. Sort in descent order to obtain feature importance\n",
    "\n",
    "$\\textbf{Advantages}$:\n",
    "- Reduce overfitting\n",
    "  - if $V(X_i)=\\sigma^2$, each $X_i$ is iid, then $V(\\bar{X})=\\frac{\\sigma^2}{n}$\n",
    "- Fast training and prediction\n",
    "- Probabilistic classification\n",
    "\n",
    "$\\textbf{Disadvantages}$:\n",
    "- Not easily interpretable (black-box)\n",
    "- Large number of trees may make the algorithm slow for real-time prediction\n",
    "- Biased in attributes with more levels, feature importance are not reliable\n",
    "- If data contains correlated features, smaller groups are more favored over larger groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting (AdaBoost)\n",
    "\n",
    "$\\textbf{Steps}$:\n",
    "- classificatoin loss function: exponential loss $=\\frac{1}{n} \\sum_{i=1}^n exp(- y_i \\hat{y_i})$\n",
    "\n",
    "1. Train a decision tree, produce an error $$r_i = \\frac{\\sum_{y_i \\neq \\hat{y_i}} w_i}{\\sum w_i}$$\n",
    "1. Compute predictor weight $$\\alpha_j = \\eta log(\\frac{1 - r_i}{r_i})$$\n",
    "1. Update instance weight, and then normalize the weight\n",
    "    $$w_i=\n",
    "    \\begin{cases}\n",
    "      w_i, & \\text{if}\\ y_i = \\hat{y_i} \\\\\n",
    "      w_i exp(\\alpha_i), & \\text{if}\\ y_i \\neq \\hat{y_i}\n",
    "    \\end{cases}$$\n",
    "1. AdaBoost predictions: compute predictions with all predictors with weights\n",
    "    $$\\hat{y}(x) = \\underset{k}{\\mathrm{argmax}} \\sum_{j=1, \\hat{y}_j (x)=k} \\alpha_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "$\\textbf{Loss function}$: \n",
    "- regression: MSE $=\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2$\n",
    "- classificaiont: cross entropy $= -[y log(\\hat{y}) + (1-y) log(1-\\hat{y}]$\n",
    "\n",
    "1st order Taylor approximation\n",
    "$$\\mathcal{L} = \\sum_i L(y_i, \\hat{y_i}^{m-1} + f_m(x_i)) \\approx \\sum_i [L(y_i, \\hat{y_i}^{m-1}) + \\frac{\\partial L}{\\partial f_m} f_m(x_i)]$$\n",
    "\n",
    "$\\textbf{Steps}$:\n",
    "1. Initial prediction: average\n",
    "2. Compute residual $y_m - \\hat{y}_{m-1}$\n",
    "3. Fit decision on residual\n",
    "4. Next prediction: \n",
    "  - regression: $f_m (x) = f_{m-1} (x) + v * h_m (x; \\theta)$\n",
    "    - $v$ is learning rate\n",
    "  - classification: $p_m (x) = \\frac{\\sum residuals}{\\sum p_{m-1}(x) (1 - p_{m-1}(x))}$\n",
    "5. When to stop: Use validation errors\n",
    "\n",
    "$\\textbf{Hyperparameters}$:\n",
    "- n_estimators: number of decision trees\n",
    "- max depth\n",
    "- common max number of leaves: 8 to 32\n",
    "- learning rate ($v$):\n",
    "  - overfitting: increase $v$\n",
    "  - underfitting: decrease $v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "![xgboost](https://miro.medium.com/max/1554/1*FLshv-wVDfu-i54OqvZdHg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Loss function}$: \n",
    "\n",
    "2st order Taylor approximation + regularization\n",
    "$$\\mathcal{L} = \\sum_i L(y_i, \\hat{y_i}^{m-1} + f_m(x_i)) + \\Omega(f_m) $$\n",
    "$$\\approx \\sum_i [L(y_i, \\hat{y_i}^{m-1}) + \\frac{\\partial L}{\\partial f_m} f_m(x_i) + \\frac{1}{2} \\frac{\\partial^2 L}{{f_m}^2} {f_m}^2 (x_i)] + \\Omega(f_m)$$\n",
    "$$\\Omega(f_m) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_j^2$$\n",
    "- T: number of leaves in the tree\n",
    "- $w_j$: leaf weights\n",
    "\n",
    "$\\textbf{Advantages}$:\n",
    "- System Optimization:\n",
    "  - Parallel Programming (pre-sorted features)\n",
    "  - Tree Pruning: don't need to reach max depth in every iteration\n",
    "  - Hardware optimization\n",
    "- Algorithmic Enhancement:\n",
    "  - Regularization on weights and leaves\n",
    "  - Sparisity Awareness: automatically 'learn' best missing value depending on training loss\n",
    "  - Weighted Quantile Sketch: find optimal splits among weighted datasets\n",
    "  - Cross-validation: built-in cross-validation at each iteration\n",
    "  \n",
    "$\\textbf{Hyperparameters}$: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "- n_estimators\n",
    "- min_child_weight\n",
    "- max_depth\n",
    "- gamma\n",
    "- subsample\n",
    "- colsample_bytree,\n",
    "- reg_alpha: L1 regularizaiton\n",
    "- reg_lambda: L2 regularization\n",
    "- learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- XGBoost: https://arxiv.org/pdf/1603.02754.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
