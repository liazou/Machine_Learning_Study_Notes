{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM\n",
    "\n",
    "- Goal: Find a vector to the hyperplane that maximizes the margin\n",
    "  - Hyperplane:\n",
    "    - Positive $H_{positive}: w x + b = 1$\n",
    "    - Negative $H_{negative}: w x + b = -1$\n",
    "  - Margin: $\\hat{\\gamma}$\n",
    "- Support Vectors: data points that are closest to the hyperplane and influence the position / orientation of the pyperplane\n",
    "- Loss function: Hinge Loss\n",
    "$$\\mathcal{L} = \\sum_{i=1}^n max(0, 1 - y_i (w x_i + b))$$\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/921/1*06GSco3ItM3gwW2scY6Tmg.png\" alt=\"SVM\"  style=\"width:600px;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Derivations}$:\n",
    "- Primal:\n",
    "$$\\underset{\\gamma}{\\mathrm{max}} \\frac{\\hat{\\gamma}}{||w||} s.t. y_i (w x_i + b) \\geq \\hat{\\gamma}$$\n",
    "- Rescale $\\hat{\\gamma}=1$, transform to convex optimization:\n",
    "$$\\underset{w, b}{\\mathrm{min}} \\frac{1}{2} ||w||^2 s.t. 1 -  y_i (w x_i + b) \\leq 0$$\n",
    "- Lagrange Multiplier:\n",
    "$$L_p (w, b, a_i) = \\frac{1}{2} ||w||^2 + \\sum_i a_i [1 -  y_i (w x_i + b)]$$\n",
    "$$\\frac{\\partial L}{\\partial w} = w - \\sum_i a_i y_i x_i = 0, \\frac{\\partial L}{\\partial b} = -\\sum_i a_i y_i = 0$$\n",
    "- Plug in $L_p$, becomes a dual problem:\n",
    "$$\\underset{\\gamma}{\\mathrm{min}} L_D (w, b, a_i) = \\underset{\\gamma}{\\mathrm{min}} \\left( \\frac{1}{2} \\sum_{i, j} a_i a_j y_i y_j \\langle x_i, x_j \\rangle - \\sum_i a_i \\right)$$\n",
    "$$ s.t. \\sum_i a_i y_i = 0, a_i \\geq 0$$\n",
    "  - transform the problem by computing the inner product\n",
    "  - can solve non-linear separable problem by kernel trick\n",
    "- Decision function:\n",
    "$$f(x) = sign \\left(\\sum_{i=1}^N a_i \\tilde{y}_i x_i x + b \\right)$$\n",
    "\n",
    "$\\textbf{Sequential Minimum Optimization (SMO)}$:\n",
    "- choose some pair $a_i, a_j$, keep other $a_k$'s fixed\n",
    "- optimize $L_D (w, b, a_i, a_j)$ w.r.t $a_i, a_j$\n",
    "- repeat until convergence\n",
    "\n",
    "$\\textbf{Equivalent Optimization Problem}$:\n",
    "- Hard Margin:\n",
    "$$\\underset{w, b}{\\mathrm{min}} \\frac{1}{2} ||w||^2 s.t. y_i (w x_i + b) \\geq 1$$\n",
    "- Soft Margin:\n",
    "$$\\underset{w, b}{\\mathrm{min}} \\left[\\frac{1}{2} ||w||^2 + C \\sum_{i=1}^n \\xi^{(i)} \\right] s.t. y_i (w x_i + b) \\geq 1 - \\xi^{(i)}$$\n",
    "$$\\underset{w, b}{\\mathrm{min}} \\left[\\frac{1}{n}\\sum_{i=1}^n max(0, 1 - y_i (w x_i + b)) + \\lambda ||w||^2 \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel SVM\n",
    "\n",
    "$\\textbf{Mercer's Theorem}$: If K is positive semi-definite and symmetric, then exists a kernal $\\phi$ such that $K(a, b) = \\phi (a)^T \\phi (b)$\n",
    "\n",
    "$\\textbf{Kernel trick}$: \n",
    "- map the original input to a new feature space\n",
    "- replace inner product $ x_i \\cdot x_j $ by kernel $ \\phi (x_i) \\cdot \\phi (x_j) $\n",
    "- train linear SVM in a new feature space\n",
    "$$L(w, b, a_i) = \\frac{1}{2} \\sum_{i, j} a_i a_j y_i y_j \\langle x_i, x_j \\rangle - \\sum_i a_i$$\n",
    "- Decision function:\n",
    "$$f(x) = sign \\left(\\sum_{i=1}^N a_i \\tilde{y}_i K(x_i, x) + b \\right)$$\n",
    "\n",
    "$\\textbf{Different Types of Kernel}$:\n",
    "- linear: $K(a, b) = a^T b$\n",
    "- polynomial: $K(a, b) = (\\gamma a^T b + r)^d$\n",
    "- Gaussian rbf: $K(a, b) = exp \\left( \\frac{||a - b||^2}{2 \\sigma^2} \\right)$\n",
    "- sigmoid: $K(a, b) = tanh(\\gamma a^T b + r)$\n",
    "\n",
    "$\\textbf{Hyperparameters}$:\n",
    "- gamma $\\gamma$: increasing gamma leads to overfitting as the classifier tries to perfectly fit the training data\n",
    "- C (equivalent to $1 / \\lambda$): \n",
    "  - C is larger, the margin will be smaller\n",
    "  - C is smaller, the margin will be larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR)\n",
    "\n",
    "$\\textbf{Objective}$:\n",
    "$$\\underset{w, b}{\\mathrm{min}} \\left[\\frac{1}{2} ||w||^2 + C \\sum_{i=1}^n L_{\\epsilon} (w x_i + b - y_i) \\right]$$\n",
    "$$L_{\\epsilon} (z) = max(|z| - \\epsilon, 0)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Domain Description (SVDD): One-class SVM\n",
    "\n",
    "$\\textbf{Objective}$: find a smallest sphere with center $o$, radius $R$ to include all positive samples\n",
    "$$\\underset{R, o, \\xi}{\\mathrm{min}} L(R, o, \\xi) = \\underset{R, o, \\xi}{\\mathrm{min}} \\left[ R^2 + C \\sum_{i=1}^n \\xi_i \\right]$$\n",
    "$$s.t. ||x_i 0 o||^2 \\leq R^2 + \\xi_i, \\xi_i \\geq 0$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Advantages}$:\n",
    "- Fast prediction\n",
    "- Fit high-dimensional data\n",
    "- kernel function adapts to many types of data\n",
    "\n",
    "$\\textbf{Disadvantages}$:\n",
    "- Computational cost (with large training samples)\n",
    "- can only fit small sample size (smaller than 1 million)\n",
    "- Strongly dependent on softening parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
