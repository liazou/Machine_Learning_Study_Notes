{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5398552-0380-4c1d-94b5-3eb83c8d736e",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Type of clustering algorithms:\n",
    "- hard clustering (k-means): clusters do not overlap\n",
    "- soft clustering (EM): clusters may overlap with probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e735bf8-5c78-4888-844f-b1e0f996ebf9",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "Objective: Given $D = \\{ x_1, x_2, ..., x_N \\}$, find k clsuters $C = \\{C_1, C_2, ..., C_k \\}$ to minimize\n",
    "\n",
    "$$\\min_{C} \\sum_{k=1}^K \\sum_{x_i \\in C_k} ||x_i - \\mu_k||^2$$\n",
    "$$\\mu_k = \\frac{1}{|C_k|} \\sum_{x_i \\in C_k} x_i$$\n",
    "\n",
    "Steps:\n",
    "- initialize k random centroids $\\{ \\mu_1, \\mu_2, ..., \\mu_k \\}$\n",
    "- for each data point $\\{ x_1, x_2, ..., x_N \\}$, find the closest centrod\n",
    "- for each cluster $\\{C_1, C_2, ..., C_k \\}$, recompute centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee98743-b59f-4adc-bef3-e42f56139c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_dist(x1, x2):\n",
    "    return sum((x1 - x2) * (x1 - x2))\n",
    "\n",
    "def kmeans(X, k, num_iter=100):\n",
    "    \"\"\"\n",
    "    Implement kmeans algorithm\n",
    "    @X: numpy array\n",
    "    @k: int (number of clusters)\n",
    "    @num_iter: int (number of iterations)\n",
    "    \"\"\"\n",
    "    # choose random datapoint as initial centroids\n",
    "    index = np.random.choice(len(X), size=k, replace=False)\n",
    "    centroids = X[index, :]\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        distances = []\n",
    "        \n",
    "        # compute distance between point and each centroid\n",
    "        for i in range(k):\n",
    "            distance = [compute_dist(x, centroids[i]) for x in X]\n",
    "            distances.append(distance)\n",
    "        \n",
    "        # find the closest centrod for each data point\n",
    "        min_distance = np.array([np.argmin(x) for x in np.array(distances).T])\n",
    "        \n",
    "        # compute new centroids\n",
    "        new_centroids = []\n",
    "        for i in range(k):\n",
    "            centroid = X[min_distance==i, :].mean(axis=0)\n",
    "            # print(centroid)\n",
    "            new_centroids.append(centroid)\n",
    "        centroids = np.array(new_centroids)\n",
    "        \n",
    "    return min_distance, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e66a55a-87e0-4d14-91c3-699f01dfc9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 2, 1, 1, 4, 4, 1, 4, 1, 4, 3, 4, 1, 0, 4, 2, 2, 2, 2, 1, 0,\n",
       "        2, 2, 1, 2, 2, 1, 0, 3, 1, 3, 0, 4, 1, 0, 3, 1, 3, 2, 1, 4, 0, 0,\n",
       "        3, 2, 1, 2, 2, 0]),\n",
       " array([[0.90634388, 0.56382264],\n",
       "        [0.24078613, 0.33989064],\n",
       "        [0.70170277, 0.83013909],\n",
       "        [0.09067113, 0.82331057],\n",
       "        [0.65647735, 0.20463219]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans(np.random.random((50, 2)), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637406ca-3878-4e4e-9c1e-4392b8cb55ec",
   "metadata": {},
   "source": [
    "# Expectation Maximization (EM)\n",
    "\n",
    "Objective: Let $Y = \\{ y_1, y_2, ..., y_N \\}$ be observed variable, $Z = \\{ z_1, z_2, ..., z_N \\}$ be hidden variable, maximize likelihood \n",
    "\n",
    "$$L(\\theta) = \\sum_{i=1}^N log p(y_i; \\theta) = \\sum_{i=1}^N log (\\sum_z p(y_i, z; \\theta)) = \\sum_{i=1}^N log (\\sum_z p(y_i | z; \\theta)p(z; \\theta))$$\n",
    "\n",
    "Steps:\n",
    "- start with randomly placed parameters\n",
    "- E step: compute expectation based on posterior probability\n",
    "\n",
    "$$Q(\\theta, \\theta^{(i)}) = \\sum_{j=1}^N E_{P(Z|y_k; \\theta^{(i)})} log P(y_j, Z; \\theta) = \\sum_{j=1}^N (\\sum_Z P(Z|y_j; \\theta^{(i)}) logP(y_j, Z;\\theta) )$$\n",
    "\n",
    "- M step: find $\\theta^{(i+1)}$ to maximize\n",
    "\n",
    "$$\\theta^{(i+1)} = \\arg \\max_{\\theta} Q(\\theta, \\theta^{(i)})$$\n",
    "\n",
    "- iterate until convergence\n",
    "  - criteria: $|\\theta^{(i+1)} - \\theta^{(i)}| < \\epsilon_1$ or $|Q(\\theta^{(i+1)}, \\theta^{(i)}) - Q(\\theta^{(i)}, \\theta^{(i)})| < \\epsilon_2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c6946-8959-40e9-a228-d8aed87288f6",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Agglomerative Hierarchical Clustering: merge data point into one cluster\n",
    "- Compute the proximity matrix\n",
    "- Let each data point be a cluster\n",
    "- Repeat: Merge two cloest clusters and updat the proximity matrix\n",
    "- Until only one single cluster remains\n",
    "\n",
    "Divisive Hierarchical Clustering: divide one cluster into single data point\n",
    "- https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec\n",
    "\n",
    "Simularity metric:\n",
    "- min linkage $\\min Sim(P_i, P_j),  \\forall P_i \\in C_1, P_j \\in C_2$\n",
    "  - pros: can separate non-elliptical shapes if the gap is not small\n",
    "  - cons: cannot separate properly if there is noise\n",
    "- max linkage $\\max Sim(P_i, P_j),  \\forall P_i \\in C_1, P_j \\in C_2$\n",
    "  - pros: does well if there is noise\n",
    "  - cons: biased towards globular clusters (Ball), tend to break large clusters\n",
    "- groupby average $\\sum Sim(P_i, P_j) / (|C_1| * |C_2|)$\n",
    "  - pros: does well if there is noise\n",
    "  - cons: biased towards globular clusters (Ball)\n",
    "- dsitance between centroids\n",
    "- ward's method $\\sum (dist(P_i, P_j))^2 / (|C_1| * |C_2|)$\n",
    "  - pros: does well if there is noise\n",
    "  - cons: biased towards globular clusters (Ball)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c7620-d9a3-416a-9ffd-65d2f02b3acc",
   "metadata": {},
   "source": [
    "# Clustering Evaluation\n",
    "\n",
    "$$\\text{Calinski Harabasz Index} = \\frac{SS_B * (N - k)}{SS_W * (k - 1)}$$\n",
    "- $SS_B$ = between cluster variation\n",
    "- $SS_W$ = within cluster variation\n",
    "- $\\frac{SS_B}{SS_W}$ should be largest at the optimal clustering size\n",
    "- http://ethen8181.github.io/machine-learning/clustering_old/clustering/clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b95f7-975c-42ad-859d-d68862a68d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
