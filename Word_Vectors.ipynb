{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram\n",
    "\n",
    "$\\textbf{Assumption}$:\n",
    "- Markov Property: only last N words matter\n",
    "- e.g. for unigram model\n",
    "$$P(\\text{teacher drinks tea}) = P(\\text{teacher}) * P(\\text{drinks|teacher}) * \\textbf{P(tea|teacher drinks})$$\n",
    "$$ = P(\\text{teacher}) * P(\\text{drinks|teacher}) * \\textbf{P(tea|teacher})$$\n",
    "\n",
    "$\\textbf{Input}$:\n",
    "- Sentence\n",
    "- Start of sentence symbols $\\text{<s>}$\n",
    "- End of sentence symbods $\\text{</s>}$\n",
    "- N-gram: add N-1 start symbols\n",
    "\n",
    "$$p(w_n | w_{n-N+1} ^{n-1}) = \\frac{Count(w_{n-N+1} ^{n-1} w_n)}{ Count(w_{n-N+1} ^{n-1}) }$$\n",
    "\n",
    "$\\textbf{Steps}$:\n",
    "1. Choose sentence start\n",
    "2. Choose next n-gram starting with previous words\n",
    "3. Continue until  $\\text{</s>}$ is picked\n",
    "\n",
    "$\\textbf{Smoothing}$:\n",
    "- Add-one\n",
    "$$p(w_n | w_{n-N+1} ^{n-1}) = \\frac{Count(w_{n-N+1} ^{n-1} w_n) + 1}{ \\sum_{w \\in V} (Count(w_{n-N+1} ^{n-1}, w) + 1) } = \\frac{Count(w_{n-N+1} ^{n-1} w_n) + 1}{ Count(w_{n-N+1} ^{n-1}) + V }$$\n",
    "- Add-k\n",
    "$$p(w_n | w_{n-N+1} ^{n-1}) = \\frac{Count(w_{n-N+1} ^{n-1} w_n) + k}{ \\sum_{w \\in V} (Count(w_{n-N+1} ^{n-1}, w) + k) } = \\frac{Count(w_{n-N+1} ^{n-1} w_n) + k}{ Count(w_{n-N+1} ^{n-1}) + k * V }$$\n",
    "\n",
    "$\\textbf{n-gram model evaluation}: \\underline{Perplexity}$\n",
    "- small perplexity means better model\n",
    "- PP(Character Level Models) < PP(Word-based Models)\n",
    "\n",
    "$$PP(W) = \\sqrt[^N]{\\prod_{i=1}^N \\frac{1}{P(w_i | w_1, ..., w_{i-1})}}$$\n",
    "$$log(PP(W)) = -\\frac{1}{N} \\sum_{i=1}^N log(P(w_i | w_1, ..., w_{i-1}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "$$TF(t) = \\frac{\\text{# times term t appears in a document}}{\\text{total # terms in the document}}$$\n",
    "$$IDF(t) = ln \\left(\\frac{\\text{total # documents}}{\\text{# documents with term t}} \\right)$$\n",
    "$$weight(t) = TF(t) * IDF(t)$$\n",
    "\n",
    "$\\textbf{Notes}$:\n",
    "- TF(t) considers $\\underline{local}$ feature in a document\n",
    "- IDF(t) contains $\\underline{global}$ feature in a document\n",
    "  - If a term appears in most of the documents, IDF(t) will be small"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "$$ e_{man} - e_{women} \\approx e_{king} - e_{queen} $$\n",
    "$$\\underset{w}{\\mathrm{argmax}} sim(e_w, e_{king} - e_{man} + e_{woman})$$\n",
    "$$\\text{cos sim}(u, w) = \\frac{u^T v}{||u||_2 * ||v||_2}$$\n",
    "\n",
    "## Word2Vec (Google)\n",
    "\n",
    "$\\textbf{Continuous bag-of-words (CBOW)}$: input previous word, output next word\n",
    "- $u_j$: score of $word_j$ in Vocabulary $V$\n",
    "- $\\overrightarrow{w}_j$: output vector of $word_j$\n",
    "\n",
    "$$E(word_1, word_2) = -log \\frac{exp(u_j)}{\\sum_{j=1}^V exp(u_j)} = -\\overrightarrow{w}_j^T \\overrightarrow{w}_I + log \\left(\\sum_{j=1}^V exp(\\overrightarrow{w}_j^T \\overrightarrow{w}_I) \\right)$$\n",
    "\n",
    "$$\\text{Objective:} \\underset{w}{\\mathrm{min}} \\sum_{(word_I, word_O) \\in D} \\left(-\\overrightarrow{w}_j^T \\overrightarrow{w}_I + log \\left(\\sum_{j=1}^V exp(\\overrightarrow{w}_j^T \\overrightarrow{w}_I) \\right) \\right) $$\n",
    "\n",
    "$\\textbf{Skip-gram}$: input one word, output words before or after the given word\n",
    "- works well for infrequent words\n",
    "- reduce computation: hierarchical softmax\n",
    "\n",
    "$$E = -\\sum_{c=1}^C u_j^c + C * log \\left(\\sum_{j=1}^V exp(u_j)\\right)$$\n",
    "\n",
    "$\\textbf{Negative Sampling}$: train a set of logistic regression on k negative and 1 positive samples\n",
    "- $\\overrightarrow{w}_{w_O}$: output vector of real word\n",
    "- $\\overrightarrow{w}_{j}$: output vector of negative sampling\n",
    "- $\\sigma (\\overrightarrow{w}_{w_O}^T \\overrightarrow{h})$: probability of predicting $w_o$ as positive\n",
    "- $\\sigma (-\\overrightarrow{w}_{j}^T \\overrightarrow{h})$: probability of predicting word j as negative\n",
    "\n",
    "$$E = -log(\\sigma (\\overrightarrow{w}_{w_O}^T \\overrightarrow{h})) - \\sum_{j\\in W_{neg}} log(\\sigma (-\\overrightarrow{w}_{j}^T \\overrightarrow{h}))$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word2Vec](https://miro.medium.com/max/680/0*TY9nYgPpwJloevhp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe (Stanford)\n",
    "\n",
    "$\\textbf{Summary}$: use neural methods to decompose co-occurrence matrix, $\\underline{global}$ vectors for word representation, aggregated global word-word co-occurrence probabilities from a corpus\n",
    "\n",
    "$\\textbf{Input}$:\n",
    "- $X_i = \\sum_{k=1}^V X_{i,k}$: total number of words that appear before and after $word_i$\n",
    "- $P_{i,j} = P(word_j | word_i) = \\frac{X_{i,j}}{X_i}$: probability that $word_j$ appears after or before $word_i$\n",
    "- $Ratio_{i,j}^k = \\frac{P_{i,k}}{P_{j,k}}$: ratio of probability\n",
    "\n",
    "$$Objective: F(\\overrightarrow{w}_i^T \\overrightarrow{w}_k - \\overrightarrow{w}_j^T \\overrightarrow{w}_k) = exp(\\overrightarrow{w}_i^T \\overrightarrow{w}_k - \\overrightarrow{w}_j^T \\overrightarrow{w}_k) = \\frac{P_{i,k}}{P_{j,k}}$$\n",
    "\n",
    "$$Loss: J = \\sum_{i, k} f(X_{i, k}) (\\overrightarrow{w}_i^T \\overrightarrow{w}_k + b_i + b_k -log(X_{i, k}))^2$$\n",
    "$$\\text{Weight function:} f(x) =\n",
    "    \\begin{cases}\n",
    "      \\left(\\frac{x}{x_{max}}\\right)^{\\alpha}, & \\text{if}\\ x < x_{max} \\\\\n",
    "      1, & \\text{else}\\ \n",
    "    \\end{cases}$$\n",
    "    \n",
    "$\\textbf{Property of f(x)}$:\n",
    "- $f(0) = 0$ so that $\\lim_{x \\to 0} f(x) log^2 x$ exists\n",
    "- f(x) is non-decreasing\n",
    "- for large $X_{i,k}$, f($X_{i,k}$) cannot be large\n",
    "\n",
    "$\\textbf{Model Evaluation Task}$:\n",
    "- Word analogies task: \"Athens\" to \"Greece\" as \"Berlin\" to \"_\"?\n",
    "- Word Similarity\n",
    "- Entity Recognition\n",
    "\n",
    "$\\textbf{Hyperparameters of GloVe}$:\n",
    "- Word Vector Length\n",
    "- Window Size of sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText (Facebook)\n",
    "\n",
    "$\\textbf{Summary}$: train neural networks in text classification\n",
    "- Input: All words in a document\n",
    "- Output: Probability of each class (y: real label)\n",
    "  - number of class K << dictionary size V\n",
    "  - don't need hierarchical softmax\n",
    "  - don't need negative sampling (less training time)\n",
    "- Hidden Vector: average of allinput words\n",
    "$$\\overrightarrow{h} = \\frac{1}{C} W^T (\\overrightarrow{x}_1 + \\overrightarrow{x}_2 + ... + \\overrightarrow{x}_C)$$\n",
    "\n",
    "$$Loss: E = -u_y + log \\left(\\sum_{k=1}^K exp(u_k)\\right) = -\\overrightarrow{w}_y^T \\overrightarrow{h} + log \\sum_{k=1}^K exp(\\overrightarrow{w}_k^T \\overrightarrow{h})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- FastText: https://arxiv.org/pdf/1607.04606.pdf\n",
    "- GloVe: https://nlp.stanford.edu/pubs/glove.pdf\n",
    "- Word2Vec: https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
