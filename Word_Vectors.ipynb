{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram\n",
    "\n",
    "$\\textbf{Assumption}$:\n",
    "- Markov Property: only last N words matter\n",
    "- e.g. for unigram model\n",
    "$$P(\\text{teacher drinks tea}) = P(\\text{teacher}) * P(\\text{drinks|teacher}) * \\textbf{P(tea|teacher drinks})$$\n",
    "$$ = P(\\text{teacher}) * P(\\text{drinks|teacher}) * \\textbf{P(tea|teacher})$$\n",
    "\n",
    "$\\textbf{Input}$:\n",
    "- Sentence\n",
    "- Start of sentence symbols $\\text{<s>}$\n",
    "- End of sentence symbods $\\text{</s>}$\n",
    "- N-gram: add N-1 start symbols\n",
    "\n",
    "$$p(w_n | w_{n-N+1} ^{n-1}) = \\frac{Count(w_{n-N+1} ^{n-1} w_n)}{ Count(w_{n-N+1} ^{n-1}) }$$\n",
    "\n",
    "$\\textbf{Steps}$:\n",
    "1. Choose sentence start\n",
    "2. Choose next n-gram starting with previous words\n",
    "3. Continue until  $\\text{</s>}$ is picked\n",
    "\n",
    "$\\textbf{Smoothing}$:\n",
    "- Add-one\n",
    "$$p(w_n | w_{n-N+1} ^{n-1}) = \\frac{Count(w_{n-N+1} ^{n-1} w_n) + 1}{ \\sum_{w \\in V} (Count(w_{n-N+1} ^{n-1}, w) + 1) } = \\frac{Count(w_{n-N+1} ^{n-1} w_n) + 1}{ Count(w_{n-N+1} ^{n-1}) + V }$$\n",
    "- Add-k\n",
    "$$p(w_n | w_{n-N+1} ^{n-1}) = \\frac{Count(w_{n-N+1} ^{n-1} w_n) + k}{ \\sum_{w \\in V} (Count(w_{n-N+1} ^{n-1}, w) + k) } = \\frac{Count(w_{n-N+1} ^{n-1} w_n) + k}{ Count(w_{n-N+1} ^{n-1}) + k * V }$$\n",
    "\n",
    "$\\textbf{n-gram model evaluation}: \\underline{Perplexity}$\n",
    "- small perplexity means better model\n",
    "- PP(Character Level Models) < PP(Word-based Models)\n",
    "\n",
    "$$PP(W) = \\sqrt[^N]{\\prod_{i=1}^N \\frac{1}{P(w_i | w_1, ..., w_{i-1})}}$$\n",
    "$$log(PP(W)) = -\\frac{1}{N} \\sum_{i=1}^N log(P(w_i | w_1, ..., w_{i-1}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "$$TF(t) = \\frac{\\text{# times term t appears in a document}}{\\text{total # terms in the document}}$$\n",
    "$$IDF(t) = ln \\left(\\frac{\\text{total # documents}}{\\text{# documents with term t}} \\right)$$\n",
    "$$weight(t) = TF(t) * IDF(t)$$\n",
    "\n",
    "$\\textbf{Notes}$:\n",
    "- TF(t) considers $\\underline{local}$ feature in a document\n",
    "- IDF(t) contains $\\underline{global}$ feature in a document\n",
    "  - If a term appears in most of the documents, IDF(t) will be small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "$$ e_{man} - e_{women} \\approx e_{king} - e_{queen} $$\n",
    "$$\\underset{w}{\\mathrm{argmax}} sim(e_w, e_{king} - e_{man} + e_{woman})$$\n",
    "$$\\text{cos sim}(u, w) = \\frac{u^T v}{||u||_2 * ||v||_2}$$\n",
    "\n",
    "## Word2Vec (Google)\n",
    "\n",
    "$\\textbf{Continuous bag-of-words (CBOW)}$: input previous word, output next word\n",
    "- $u_j$: score of $word_j$ in Vocabulary $V$\n",
    "- $\\overrightarrow{w}_j$: output vector of $word_j$\n",
    "\n",
    "$$E(word_1, word_2) = -log \\frac{exp(u_j)}{\\sum_{j=1}^V exp(u_j)} = -\\overrightarrow{w}_j^T \\overrightarrow{w}_I + log \\left(\\sum_{j=1}^V exp(\\overrightarrow{w}_j^T \\overrightarrow{w}_I) \\right)$$\n",
    "\n",
    "$$\\text{Objective:} \\underset{w}{\\mathrm{min}} \\sum_{(word_I, word_O) \\in D} \\left(-\\overrightarrow{w}_j^T \\overrightarrow{w}_I + log \\left(\\sum_{j=1}^V exp(\\overrightarrow{w}_j^T \\overrightarrow{w}_I) \\right) \\right) $$\n",
    "\n",
    "$\\textbf{Skip-gram}$: input one word, output words before or after the given word\n",
    "- works well for infrequent words\n",
    "- reduce computation: hierarchical softmax\n",
    "\n",
    "$$E = -\\sum_{c=1}^C u_j^c + C * log \\left(\\sum_{j=1}^V exp(u_j)\\right)$$\n",
    "\n",
    "$\\textbf{Negative Sampling}$: train a set of logistic regression on k negative and 1 positive samples\n",
    "- $\\overrightarrow{w}_{w_O}$: output vector of real word\n",
    "- $\\overrightarrow{w}_{j}$: output vector of negative sampling\n",
    "- $\\sigma (\\overrightarrow{w}_{w_O}^T \\overrightarrow{h})$: probability of predicting $w_o$ as positive\n",
    "- $\\sigma (-\\overrightarrow{w}_{j}^T \\overrightarrow{h})$: probability of predicting word j as negative\n",
    "\n",
    "$$E = -log(\\sigma (\\overrightarrow{w}_{w_O}^T \\overrightarrow{h})) - \\sum_{j\\in W_{neg}} log(\\sigma (-\\overrightarrow{w}_{j}^T \\overrightarrow{h}))$$\n",
    "\n",
    "## GloVe (Stanford)\n",
    "\n",
    "## FastText (Facebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
