{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "## Regression\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$$MSE = \\sqrt{\\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "$$MAE = \\frac{1}{n} \\sum_i |y_i - \\hat{y}_i|$$\n",
    "\n",
    "### R-squared\n",
    "\n",
    "$$R^2 = 1 - \\frac{SSE}{SST} = \\frac{SSR}{SST}$$\n",
    "$$SSE = \\sum_i (y_i - \\hat{y}_i)^2, SST = \\sum_i (y_i - \\bar{y})^2, SSR = \\sum_i (\\hat{y}_i - \\bar{y})^2$$\n",
    "$$SST = SSE + SSR$$\n",
    "\n",
    "### Adjusted R-squared\n",
    "- k: model degree of freedom = # parameters\n",
    "- N: number of observations\n",
    "\n",
    "$$R_{adjusted}^2 = 1 - \\frac{(1 - R^2) * (N - 1)}{N - K - 1} < R^2$$\n",
    "\n",
    "$\\textbf{When feature increases}$:\n",
    "- $R^2$ is non-decreasing\n",
    "- $R_{adjusted}^2$ may decrease\n",
    "\n",
    "### Akaike Information Criterion (AIC)\n",
    "- L: Likelihood\n",
    "  - SSE: Normal Distribution\n",
    "  - SAE: Laplace Distribution\n",
    "  - Cross Entropy: Bernoulli Distribution\n",
    "  \n",
    "$$AIC = 2*k - 2 ln(L)$$\n",
    "\n",
    "### Bayesian Information Criterion (BIC)\n",
    "\n",
    "$$BIC = ln(N)*k - 2 ln(L)$$\n",
    "\n",
    "### AICc\n",
    "\n",
    "$$AICc = AIC + \\frac{2k + 2k^2}{N - k - 1}$$\n",
    "\n",
    "$\\textbf{Comparison}$:\n",
    "- AIC: more penalty on parameters\n",
    "- BIC: more penalty on observations (may under-fitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "<img src=\"https://research.aimultiple.com/wp-content/uploads/2019/07/positive-negative-true-false-matrix.png\" alt=\"classification\" width=500>\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$Accuracy = \\frac{True Positive + True Negative}{True Positive + True Negative + False Positive + False Negative}$$\n",
    "\n",
    "### AUC-ROC\n",
    "- AUC: Area under curve\n",
    "- ROC: Receiver operating characteristic\n",
    "- vertical-axis: True Positive Rate (TPR) = Recall\n",
    "- horizontal-axis: False Positive Rate (FPR)\n",
    "$$FPR = \\frac{False Positive}{Actual Negative} = \\frac{False Positive}{False Positive + True Negative}$$\n",
    "- closer to point (0, 1), better the curve\n",
    "\n",
    "<img src=\"https://glassboxmedicine.files.wordpress.com/2019/02/roc-curve-v2.png?w=576\" alt=\"AUC-ROC\" width=400>\n",
    "\n",
    "- $f_1 (x)$: probability instace class belongs to positive\n",
    "- $f_0 (x)$: otherwise\n",
    "- $X_1$: score for a positive instance\n",
    "- $X_0$: score for a negative instance\n",
    "\n",
    "$$TPR(T) = \\int_T^\\infty f_1 (x) dx, FPR(T) = \\int_T^\\infty f_0 (x) dx$$\n",
    "$$Area = \\int_{x=0}^1 TPR(FPR^{-1}(x)) dx = P(X_1 > X_0)$$\n",
    "\n",
    "$\\textbf{Why ROC is not good for imbalanced data}$: False positive rate does not drop drastically when total real negative samples are huge.\n",
    "\n",
    "### Recall\n",
    "\n",
    "$$Recall = \\frac{True Positive}{Actual Positive} = \\frac{True Positive}{True Positive + False Negative}$$\n",
    "\n",
    "### Specificity\n",
    "\n",
    "$$Specificity = \\frac{True Negative}{Actual Negative} = \\frac{True Negative}{True Negative + False Positive}$$\n",
    "\n",
    "### Precision\n",
    "\n",
    "$$Precision = \\frac{True Positive}{Predicted Positive} = \\frac{True Positive}{True Positive + False Positive}$$\n",
    "\n",
    "### $F_1$ Score\n",
    "- harmonic average of precision and recall\n",
    "\n",
    "$$F_1 = \\frac{2}{\\frac{1}{Recall} + \\frac{1}{Precision}} = 2 * \\frac{Precision * Recall}{Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation-Test Split\n",
    "\n",
    "## Training Set\n",
    "\n",
    "- Needs the larger proportion for training model\n",
    "\n",
    "## Validation Set\n",
    "\n",
    "- Hyperparameter search\n",
    "- Cannot be too small since it will cause overfitting\n",
    "\n",
    "## Testing Set\n",
    "\n",
    "- Evaluates model predictability\n",
    "\n",
    "Machine Learning (Small amount of data, < million):\n",
    "- Without testing set: Training (70%), Validation (30%)\n",
    "- Include testing: Training (60%), Validation (20%), Testing (20%)\n",
    "\n",
    "Deep Learning (Large amount of data, > million):\n",
    "- No Validation: Training (98%), Testing (2%)\n",
    "\n",
    "## Cross-Validation (K-fold)\n",
    "- split data in k folds\n",
    "- for each training:\n",
    "  - k-1 fold for training\n",
    "  - 1 fold for testing\n",
    "- reduce overfitting\n",
    "\n",
    "## Walk-Forward Optimization (Time Series)\n",
    "\n",
    "$\\textbf{Regular}$:\n",
    "- fix a window size (k)\n",
    "- for each window:\n",
    "  - for k period for training\n",
    "  - next 1 period of testing\n",
    "- need to consider sequential order\n",
    "\n",
    "$\\textbf{Anchored}$:\n",
    "- for each training:\n",
    "  - include the previous period for training\n",
    "  - next 1 period of testing\n",
    "\n",
    "<img align=\"left\" src=\"https://www.amibroker.com/guide/gifs/walkfwd2.gif\" alt=\"nested walk forward optimization\" width=460>\n",
    "\n",
    "<img align=\"right\" src=\"https://www.multicharts.com/trading-software/images/4/45/Wfoptimization2.png\" alt=\"nested walk forward optimization\" width=400>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
