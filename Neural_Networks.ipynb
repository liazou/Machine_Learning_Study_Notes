{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "$\\textbf{Assumption}$: training set is linearly separable\n",
    "\n",
    "$\\textbf{Loss function}$:\n",
    "$$\\mathcal{L} (w, b) = -\\sum y_i (w^T x_i + b)$$\n",
    "\n",
    "$\\textbf{Steps}$:\n",
    "- model:\n",
    "$$f(x) = sign(w^T x + b), sign(x) = \\begin{cases}\n",
    "      1, & \\text{if}\\ x > 0\\\\\n",
    "      -1, & \\text{else}\\ \n",
    "    \\end{cases}$$\n",
    "- Initialize $w_0, b_0$\n",
    "- Gradient descent:\n",
    "  - $\\alpha$: learning rate\n",
    "$$\\frac{\\partial \\mathcal{L}}{w} = - \\sum y_i x_i, \\frac{\\partial \\mathcal{L}}{b} = - \\sum y_i$$\n",
    "$$w := w + \\alpha y_i x_i, b := b + \\alpha y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network (FFNN)\n",
    "\n",
    "$\\textbf{Output Layer}$:\n",
    "- Gaussian (regression): Linear Unit\n",
    "- Bernoulli (binary classification): Sigmoid\n",
    "  - gradient vanishes with extreme negative or positive values\n",
    "- Multinomial (multilabel classification): Softmax\n",
    "  - gradient vanishes when difference between input values are extreme\n",
    "  - softmax: differentiable and continuous\n",
    "  - argmax (one-hot): not continuous / differentiable\n",
    "\n",
    "$\\textbf{Activation Functions}$:\n",
    "- Sigmoid: $\\sigma(x) = \\frac{1}{1 + exp(-x)} \\in (0, 1)$\n",
    "- Hyperbolic Tangent: $tanh(x) = \\frac{exp(x) - exp(-x)}{exp(x) + exp(-x)} \\in (-1, 1)$\n",
    "- Rectified Linear Unit: $ReLU(x) = max(0, x)$\n",
    "  - Leaky ReLU: $f(x) = \\begin{cases}\n",
    "      x, & \\text{if}\\ x > 0\\\\\n",
    "      0.01 x, & \\text{else}\\ \n",
    "    \\end{cases}$\n",
    "  - Exponential ReLU: $f(x) = \\begin{cases}\n",
    "      x, & \\text{if}\\ x > 0\\\\\n",
    "      a (exp(x) - 1), & \\text{else}\\ \n",
    "    \\end{cases}$\n",
    "  - Softplus/Smooth ReLU: $f(x) = ln(1 + exp(x))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![backpropagation](images/Neural_Networks/backpropagation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Forward propagation}$\n",
    "1. $a^{[1]} = f_1(z^{[1]}), z^{[1]} = w^{[1]} x + b^{[1]}$\n",
    "2. $\\hat{y} = a^{[2]} = f_2(z^{[2]}), z^{[2]} = w^{[2]} a^{[1]} + b^{[2]}$\n",
    "3. $loss = L(y, \\hat{y})$\n",
    "\n",
    "$\\textbf{Backpropagation}$\n",
    "1. $w^{[2]} := w^{[2]} - \\alpha * \\frac{\\partial L}{\\partial w^{[2]}} = w^{[2]} - \\alpha * \\left(\\frac{\\partial L}{\\partial f_2} * \\frac{\\partial f_2}{\\partial z^{[2]}}\\right) * \\frac{\\partial z^{[2]}}{\\partial w^{[2]}}$\n",
    "   - $da^{[2]} = \\frac{\\partial L}{\\partial f_2} * \\frac{\\partial f_2}{\\partial z^{[2]}}$\n",
    "2. $w^{[1]} := w^{[1]} - \\alpha * \\frac{\\partial L}{\\partial w^{[1]}} = w^{[1]} - \\alpha * \\left(\\frac{\\partial L}{\\partial f_2} * \\frac{\\partial f_2}{\\partial z_2} \\right) * \\left(\\frac{\\partial z_2}{\\partial f_1} * \\frac{\\partial f_1}{\\partial z^{[1]}} \\right) * \\frac{\\partial z^{[1]}}{\\partial w^{[1]}}$\n",
    "   - $da^{[1]} = da^{[2]} * \\frac{\\partial z^{[2]}}{\\partial f_1} * \\frac{\\partial f_1}{\\partial z^{[1]}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Gradient Descent Algorithms}$:\n",
    "- General Gradient Descent:\n",
    "  - Hyperparameter\n",
    "    - $\\alpha$: learning rate\n",
    "    $$w := w - \\alpha * \\frac{\\partial L}{\\partial w}$$\n",
    "- Momentum (exponential weighted average)\n",
    "  - Hyperparameter\n",
    "    - $\\alpha$: learning rate\n",
    "    - $\\beta$: decay (default 0.9)\n",
    "    $$V_{dw} = \\beta_1 V_{dw} + (1 - \\beta_1) dw, V_{db} = \\beta_1 V_{db} + (1 - \\beta_1) db$$\n",
    "    $$w := w - \\alpha V_{dw}, b := b - \\alpha V_{db}$$\n",
    "- AdaGrad (adaptive gradient)\n",
    "  - compute outer product of gradient, sum up diagonal\n",
    "  - Hyperparameter\n",
    "    - $\\alpha$: learning rate\n",
    "    - $\\epsilon$: small positive number\n",
    "    $$G_{dw} := G_{dw} + dw^2, G_{db} := G_{db} + db^2$$\n",
    "    $$w := w - \\alpha \\frac{dw}{\\sqrt{G_{dw} + \\epsilon}}, b := b - \\alpha \\frac{db}{\\sqrt{G_{db} + \\epsilon}}$$\n",
    "- RMSprop\n",
    "  - Hyperparameter\n",
    "    - $\\alpha$: learning rate (default 0.001)\n",
    "    - $\\beta$: decay (default 0.9)\n",
    "    - $\\epsilon$: small positive number (default $10^{-8}$)\n",
    "    $$S_{dw} = \\beta_2 S_{dw} + (1 - \\beta_2) dw^2, S_{db} = \\beta_1 V_{db} + (1 - \\beta_2) db^2$$\n",
    "    $$w := w - \\alpha \\frac{dw}{\\sqrt{S_{dw} + \\epsilon}}, b := b - \\alpha \\frac{db}{\\sqrt{S_{db} + \\epsilon}}$$\n",
    "- Adam (Adaptive Momentum) optimizer\n",
    "  - Hyperparameter\n",
    "    - $\\alpha$: learning rate\n",
    "    - $\\beta_1$: decay for $dw$\n",
    "    - $\\beta_2$: decay for $dw^2$\n",
    "    - $\\epsilon$: small positive number (default $10^{-8}$)\n",
    "    $$V_{dw} = \\beta_1 V_{dw} + (1 - \\beta_1) dw, V_{db} = \\beta_1 V_{db} + (1 - \\beta_1) db$$\n",
    "    $$S_{dw} = \\beta_2 S_{dw} + (1 - \\beta_2) dw^2, S_{db} = \\beta_1 V_{db} + (1 - \\beta_2) db^2$$\n",
    "    $$V_{dw}^{corrected} = \\frac{V_{dw}}{1 - \\beta_1^t}, V_{db}^{corrected} = \\frac{V_{db}}{1 - \\beta_1^t}$$\n",
    "    $$S_{dw}^{corrected} = \\frac{S_{dw}}{1 - \\beta_2^t}, S_{db}^{corrected} = \\frac{S_{db}}{1 - \\beta_2^t}$$\n",
    "    $$w := w - \\alpha \\frac{V_{dw}^{corrected}}{\\sqrt{S_{dw}^{corrected} + \\epsilon}}, b := b - \\alpha \\frac{V_{db}^{corrected}}{\\sqrt{S_{db}^{corrected} + \\epsilon}}$$\n",
    "- Learning rate decay:\n",
    "    $$\\alpha = \\frac{1}{1 + \\beta * \\text{epoch num}} \\alpha_0$$\n",
    "    $$\\alpha = \\frac{k}{\\sqrt{\\text{epoch num}}} \\alpha_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Batch Size vs Accuracy vs GPU}$\n",
    "\n",
    "| Batch Size | Converage | Gradient estimate | Generalization | GPU memory |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Small | Slowly | Noisier | Good | Less |\n",
    "| Large | Quickly | More accurate | Bad | More |\n",
    "\n",
    "$\\textbf{Training Slow}$\n",
    "- Large sample size: modify batch size\n",
    "- Gradient small: increate learning rate\n",
    "\n",
    "$\\textbf{Batch Normalization (BN)}$\n",
    "- Steps:\n",
    "  1. compute mini-batch mean: $\\mu_{B} = \\frac{1}{m} \\sum_{i=1}^m x_i$\n",
    "  2. compute mini-batch variance:$\\sigma_{B}^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2$\n",
    "  3. normalize: $\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_{B}^2+ \\epsilon} }$\n",
    "  4. scale and shift: $y_i = \\gamma \\hat{x_i} + \\beta$\n",
    "\n",
    "- Advantages:\n",
    "  - keep the distribution of X consistent (mean 0, variance 1)\n",
    "  - allow each layer to learn by itself more independently of other layers\n",
    "  - can use higher learning rate\n",
    "  - reduce overfitting: add some noise to each hidden layer\n",
    "  \n",
    "- Disadvantages:\n",
    "  - need to have large batch size\n",
    "  - not suitable for RNN\n",
    "    - RNNâ€™s depth is unstable\n",
    "    - hard to normalize the same layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Reduce Overfitting}$:\n",
    "- Regularization\n",
    "  - L2\n",
    "    $$\\tilde{\\mathcal{L}}(w, X, y) = \\mathcal{L}(w, X, y) + \\frac{\\lambda}{2} w^T w$$\n",
    "    $$\\frac{\\partial \\tilde{\\mathcal{L}}(w, X, y)}{\\partial w} = \\frac{\\partial \\mathcal{L}(w, X, y)}{\\partial w} + \\lambda w$$\n",
    "  - L1\n",
    "    $$\\tilde{\\mathcal{L}}(w, X, y) = \\mathcal{L}(w, X, y) + \\lambda ||w||_1$$\n",
    "    $$\\frac{\\partial \\tilde{\\mathcal{L}}(w, X, y)}{\\partial w} = \\frac{\\partial \\mathcal{L}(w, X, y)}{\\partial w} + \\lambda sign(w)$$\n",
    "- Early Stopping\n",
    "  - small learning rate leads to early stopping given same iteration\n",
    "  - not converage to local optimum in traning\n",
    "  - can reduce training cost\n",
    "- Dropout\n",
    "  - randomly deactivate a part of neurons during training (similar to bagging)\n",
    "  - don't need to dropout in testing\n",
    "- Reduce number of hidden layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
