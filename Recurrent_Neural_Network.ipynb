{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General RNN\n",
    "\n",
    "$\\textbf{Backpropagation throught time (BPTT)}$:\n",
    "$$h^{(t)} = tanh(b + W h^{(t-1)} + U x^{(t)})$$\n",
    "$$o^{(t)} = softmax(c + V h^{(t)}), L = -\\sum_{t=1}^{\\tau} \\sum_{k=1}^K I(k=y^{(t)}) log \\left({o}^{(t)}_k \\right)$$\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/4136/1*SKGAqkVVzT6co-sZ29ze-g.png\" alt=\"RNN Architecture\" style=\"width:700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{RNN Task Applications}$:\n",
    "- Many-to-One: Sentiment Analysis, Video Activity Recognition\n",
    "- Many-to-Many: Machine Translation, Speech Recognition, Name Entity Recognition\n",
    "- One-to-Many: Music Generation\n",
    "\n",
    "<img src=\"https://cbare.github.io/images/recurrent-neural-net-types.png\" alt=\"RNN types\" style=\"width:700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Handle Gradient Explosion}$:\n",
    "- Gradient Clipping: set a maximum gradient\n",
    "\n",
    "$\\textbf{Handle Gradient Vanishing}$:\n",
    "- ReLU: constant derivative\n",
    "- Batch Normalization\n",
    "  - speed up convergence\n",
    "  - control overfitting\n",
    "  - reduce dropout and regularization\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN\n",
    "\n",
    "$$h^{(t)} = tanh(b_{left} + W_{left} h^{(t-1)} + U_{left} x^{(t)})$$\n",
    "$$g^{(t)} = tanh(b_{right} + W_{right} g^{(t-1)} + U_{right} x^{(t)})$$\n",
    "$$o^{(t)} = softmax(c + V_{left} h^{(t)} + V_{right} g^{(t)})$$\n",
    "\n",
    "<img src=\"http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/imgs/dl_rnn/bi_RNN.png\" align=\"left\" alt=\"Bi-RNN\" style=\"width:500px;\"/>\n",
    "\n",
    "<img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/bidirectional-rnn-ltr.png?e3e66fae56ea500924825017917b464a\" align=\"left\" alt=\"bi-RNN-stanford\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN\n",
    "\n",
    "$$h^{(t)} = tanh(b_1 + W_1 h^{(t-1)} + U x^{(t)})$$\n",
    "$$z^{(t)} = tanh(b_2 + W_2 z^{(t-1)} + R z^{(t)})$$\n",
    "$$o^{(t)} = softmax(c + V z^{(t)})$$\n",
    "\n",
    "<img src=\"http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/imgs/dl_rnn/more_hiden.png\" align=\"left\" alt=\"Deep-RNN\" style=\"width:500px;\"/>\n",
    "\n",
    "<img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/deep-rnn-ltr.png?f57da6de44ddd4709ad3b696cac6a912\" align=\"right\" alt=\"deep-RNN-stanford\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory (LSTM)\n",
    "\n",
    "$\\textbf{Key Components}$:\n",
    "\n",
    "$$\\text{forget gate:} {f}^{(t)} = \\sigma \\left({b}^{f} + U^f {x}^{(t)} + W^f {h}^{(t-1)} \\right)$$\n",
    "\n",
    "$$\\text{input gate:} {g}^{(t)} = \\sigma \\left({b}^{i} + U^i {x}^{(t)} + W^i {h}^{(t-1)} \\right)$$\n",
    "\n",
    "$$\\text{output gate:} {q}^{(t)} = \\sigma \\left({b}^{o} + U^o {x}^{(t)} + W^o {h}^{(t-1)} \\right)$$\n",
    "\n",
    "$$\\text{cell stage:} {C}^{(t)} = {f}^{(t)} \\odot {C}^{(t-1)} + {g}^{(t)} \\odot tanh \\left({b} + U {x}^{(t)} + W {h}^{(t-1)} \\right)$$\n",
    "\n",
    "$$\\text{cell output:} {h}^{(t)} = tanh \\left({C}^{(t)} \\right) \\odot {q}^{(t)}$$\n",
    "\n",
    "$${o}^{(t)} = softmax({C}^{(t)} + V {h}^{(t)}), L = -\\sum_{t=1}^{\\tau} \\sum_{k=1}^K I(k=y^{(t)}) log \\left({o}^{(t)}_k \\right)$$\n",
    "\n",
    "<img src=\"http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/imgs/dl_rnn/lstm_1.png\" alt=\"LSTM\" style=\"width:700px;\"/>\n",
    "\n",
    "$\\textbf{Handle gradient vanishing}$:\n",
    "- memory and input are added\n",
    "- influence never disappears unless forget gate is closed\n",
    "- no gradient vanishing when forget gate is opened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Recurrent Unit (GRU)\n",
    "\n",
    "- no forget gate compared with LSTM\n",
    "- train faster than LSTM on less training data\n",
    "- simpler and easier to modify\n",
    "\n",
    "$\\textbf{Key Components}$:\n",
    "\n",
    "$$\\text{update gate:} {z}^{(t)} = \\sigma \\left({b}^{z} + U^z {x}^{(t)} + W^z {h}^{(t-1)} \\right)$$\n",
    "\n",
    "$$\\text{reset gate:} {r}^{(t)} = \\sigma \\left({b}^{r} + U^r {x}^{(t)} + W^r {h}^{(t-1)} \\right)$$\n",
    "\n",
    "$$\\text{cell output:} {h}^{(t)} = {z}^{(t)} \\odot {h}^{(t-1)} + (1 - {z}^{(t)}) \\odot tanh \\left({b} + U x^{(t)} + W r^{(t)} h^{(t-1)} \\right)$$\n",
    "\n",
    "$${o}^{(t)} = softmax({C}^{(t)} + V {h}^{(t)}), L = -\\sum_{t=1}^{\\tau} \\sum_{k=1}^K I(k=y^{(t)}) log \\left({o}^{(t)}_k \\right)$$\n",
    "\n",
    "<img src=\"http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/imgs/dl_rnn/GRU.png\" alt=\"GRU\" style=\"width:300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- RNN: https://www.coursera.org/learn/nlp-sequence-models\n",
    "- LSTM: https://www.bioinf.jku.at/publications/older/2604.pdf\n",
    "- GRU: https://arxiv.org/pdf/1406.1078.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
