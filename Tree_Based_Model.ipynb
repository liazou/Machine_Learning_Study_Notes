{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Based Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "$\\textbf{Classification}$:\n",
    "- Entropy\n",
    "$$E(p) = - \\sum_{i=1}^c p_i log_2(p_i)$$\n",
    "- Gini Impurity\n",
    "$$I_{G}(p) = \\sum_{i=1}^J p_i \\sum_{k \\neq i} p_k = \\sum_{i=1}^J p_i (1 - p_i) = 1 - \\sum_{i=1}^J p_i^2$$\n",
    "\n",
    "$\\textbf{Regression}$:\n",
    "- Variance\n",
    "$$V(S) = \\frac{1}{|S|^2}\\sum_{i \\in S} \\sum_{j \\in S} (x_i - x_j)^2$$\n",
    "\n",
    "$\\textbf{How to split node}$:\n",
    "1. Compute entropy/gini impurity of the target\n",
    "2. Dataset is split on different attributes. The entropy/gini impurity of each branch is calculated\n",
    "3. Compute information gain (difference of entropy)\n",
    "4. Choose the split with the largest information gain as the decision node\n",
    "\n",
    "$\\textbf{How to split continuous features}$:\n",
    "1. Sort values in ascent order: $a_1, a_2, ..., a_M$\n",
    "2. Choose $M-1$ splitting points: $\\frac{a_1 + a_2}{2}, \\frac{a_2 + a_3}{2}, ..., \\frac{a_{M-1} + a_M}{2}$\n",
    "3. Find the splitting point just as categorial features\n",
    "\n",
    "$\\textbf{Different types of decision tree}$: https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "- ID3: Information Gain (discrete features)\n",
    "- C.4.5: Information Gain (continuous features)\n",
    "- CART: Gini impurity\n",
    "- CHAID:\n",
    "  - Continuous response: F-test\n",
    "  - Categorical response: $\\chi^2$-test\n",
    "- MARS\n",
    "- Conditional Inference Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Regularization of Decision Tree}$: http://cs229.stanford.edu/notes/cs229-notes-dt.pdf\n",
    "- Min Leaf Size: Do not split if its cardinality falls below a fixed threshold\n",
    "- Max Depth: Do not split if more than a fixed threshold of splits were already taken to reach region\n",
    "- Max Number of Nodes: Stop if a tree has more than a fixed threshold of leaf nodes\n",
    "\n",
    "$\\textbf{Advantages}$:\n",
    "- easily interpret (white-box)\n",
    "- not sensitive to outliers\n",
    "- works well in qualitative (categorical) features\n",
    "\n",
    "$\\textbf{Disadvantages}$:\n",
    "- overfitting\n",
    "  - How to overcome overfitting\n",
    "    - Regularization\n",
    "    - Ensemble learning (bagging, boosting)\n",
    "- can only output discrete values\n",
    "- sensitive to direction of decision boundary\n",
    "  - How to resolve direction problem\n",
    "    - PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "![random_forest](https://miro.medium.com/max/592/1*i0o8mjFfCn-uD79-F1Cqkw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Steps}$:\n",
    "1. Sample data (row) with replacement\n",
    "2. Sample feature (column) without replacement\n",
    "  - bagging tree does not sample feature\n",
    "3. Build every decision tree independently\n",
    "4. Final prediction: average or majority vote\n",
    "5. $\\underline{\\textbf{Note}}$: No loss function, use out-of-bag errors\n",
    "\n",
    "$\\textbf{Hyperparameters}$:\n",
    "- max_features: max # of features\n",
    "- n_estimators: number of decision trees\n",
    "- min_samples_leaf\n",
    "\n",
    "$\\textbf{Feature Importance}$: https://blog.datadive.net/selecting-good-features-part-iii-random-forests/\n",
    "1. Each split will calculate decrease of variance/entropy\n",
    "2. Sum up total decrease of variance/entropy by feature\n",
    "3. Sort in descent order to obtain feature importance\n",
    "\n",
    "$\\textbf{Advantages}$:\n",
    "- Reduce overfitting\n",
    "  - if $V(X_i)=\\sigma^2$, each $X_i$ is iid, then $V(\\bar{X})=\\frac{\\sigma^2}{n}$\n",
    "- Fast training and prediction\n",
    "- Probabilistic classification\n",
    "\n",
    "$\\textbf{Disadvantages}$:\n",
    "- Not easily interpretable (black-box)\n",
    "- Large number of trees may make the algorithm slow for real-time prediction\n",
    "- Biased in attributes with more levels, feature importance are not reliable\n",
    "- If data contains correlated features, smaller groups are more favored over larger groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Tree\n",
    "\n",
    "$\\textbf{Loss function}$:\n",
    "- regression: MSE $=\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2$\n",
    "- classificatoin: exponential loss $=\\frac{1}{n} \\sum_{i=1}^n exp(- y_i \\hat{y_i})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting (AdaBoost)\n",
    "\n",
    "$\\textbf{Steps}$:\n",
    "1. Train a decision tree, produce an error $$r_i = \\frac{\\sum_{y_i \\neq \\hat{y_i}} w_i}{\\sum w_i}$$\n",
    "2. Compute predictor weight $$\\alpha_j = \\eta log(\\frac{1 - r_i}{r_i})$$\n",
    "3. Update instance weight, and then normalize the weight\n",
    "    $$w_i=\n",
    "    \\begin{cases}\n",
    "      w_i, & \\text{if}\\ y_i = \\hat{y_i} \\\\\n",
    "      w_i exp(\\alpha_i), & \\text{if}\\ y_i \\neq \\hat{y_i}\n",
    "    \\end{cases}$$\n",
    "4. AdaBoost predictions: compute predictions with all predictors with weights\n",
    "    $$\\hat{y}(x) = \\underset{k}{\\mathrm{argmax}} \\sum_{j=1, \\hat{y}_j (x)=k} \\alpha_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "$\\textbf{Loss function}$: 1st order Taylor approximation\n",
    "$$\\mathcal{L} = \\sum_i L(y_i, \\hat{y_i}^{m-1} + f_m(x_i)) \\approx \\sum_i [L(y_i, \\hat{y_i}^{m-1}) + \\frac{\\partial L}{\\partial f_m} f_m(x_i)]$$\n",
    "\n",
    "$\\textbf{Steps}$:\n",
    "1. Initial prediction: average\n",
    "2. Compute residual $y_m - \\hat{y}_{m-1}$\n",
    "3. Fit decision on residual\n",
    "4. Next prediction: \n",
    "  - regression: $f_m (x) = f_{m-1} (x) + v * h_m (x; \\theta)$\n",
    "    - $v$ is learning rate\n",
    "  - classification: $p_m (x) = \\frac{\\sum residuals}{\\sum p_{m-1}(x) (1 - p_{m-1}(x))}$\n",
    "5. When to stop: Use validation errors\n",
    "\n",
    "$\\textbf{Hyperparameters}$:\n",
    "- n_estimators: number of decision trees\n",
    "- max depth\n",
    "- common max number of leaves: 8 to 32\n",
    "- learning rate ($v$):\n",
    "  - overfitting: increase $v$\n",
    "  - underfitting: decrease $v$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "![xgboost](https://miro.medium.com/max/1554/1*FLshv-wVDfu-i54OqvZdHg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Loss function}$: 2st order Taylor approximation + regularization\n",
    "$$\\mathcal{L} = \\sum_i L(y_i, \\hat{y_i}^{m-1} + f_m(x_i)) + \\Omega(f_m) $$\n",
    "$$\\approx \\sum_i [L(y_i, \\hat{y_i}^{m-1}) + \\frac{\\partial L}{\\partial f_m} f_m(x_i) + \\frac{1}{2} \\frac{\\partial^2 L}{{f_m}^2} {f_m}^2 (x_i)] + \\Omega(f_m)$$\n",
    "$$\\Omega(f_m) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_j^2$$\n",
    "- T: number of nodes\n",
    "- $w_j$: output of each node\n",
    "\n",
    "$\\textbf{Advantages}$:\n",
    "- System Optimization:\n",
    "  - Parallel Programming (pre-sorted features)\n",
    "  - Tree Pruning: don't need to reach max depth in every iteration\n",
    "  - Hardware optimization\n",
    "- Algorithmic Enhancement:\n",
    "  - Regularization on weights and leaves\n",
    "  - Sparisity Awareness: automatically 'learn' best missing value depending on training loss\n",
    "  - Weighted Quantile Sketch: find optimal splits among weighted datasets\n",
    "  - Cross-validation: built-in cross-validation at each iteration\n",
    "  \n",
    "$\\textbf{Hyperparameters}$: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "- n_estimators\n",
    "- min_child_weight\n",
    "- max_depth\n",
    "- gamma\n",
    "- subsample\n",
    "- colsample_bytree,\n",
    "- reg_alpha: L1 regularizaiton\n",
    "- reg_lambda: L2 regularization\n",
    "- learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- XGBoost: https://arxiv.org/pdf/1603.02754.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
