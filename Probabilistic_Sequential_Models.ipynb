{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "$$p(y_k|x_1, ..., x_m) = p(y_k)p(x_1,...,x_m|y_k) = p(y_k) \\prod_{i=1}^m p(x_i|y_k)$$\n",
    "\n",
    "$\\textbf{Steps}$:\n",
    "\n",
    "N: number of sample, m: number of features, K: number of labels\n",
    "1. Compute prior and conditional probability\n",
    "$$p(y=c_k) = \\frac{1}{N} \\sum_{i=1}^N 1(\\hat{y}_i = c_k)$$\n",
    "$$p(x_j=a_{j, l}| y = c_k) = \\frac{\\sum_{i=1}^N 1(x_{i,j}=a_{j,l}, \\hat{y}_i = c_k)}{\\sum_{i=1}^N 1(\\hat{y}_i = c_k)}$$\n",
    "$$j=1,2,....,m;l=1,2,...,s_j;k=1,2,...,K$$\n",
    "2. Given $\\overrightarrow{x}$, compute\n",
    "$$p(y_k|x_1, ..., x_m) = p(y_k=c_k) \\prod_{i=1}^m p(x_i|y_k=c_k)$$\n",
    "3. Predict \n",
    "$$\\hat{y} = \\underset{c_k}{\\mathrm{argmax}} [p(y_k=c_k) \\prod_{i=1}^m p(x_i|y_k=c_k)]$$\n",
    "\n",
    "$\\textbf{Assumptions}$:\n",
    "- Features are conditoinally independent\n",
    "\n",
    "$\\textbf{Advantages}$:\n",
    "- Fase for training and prediction\n",
    "- Easy to interpret (white box)\n",
    "- Few tunable hyperparameters\n",
    "- Less complexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models (HMM)\n",
    "\n",
    "$\\textbf{Assumptions}$:\n",
    "- Independence: labels are independent\n",
    "  - What if labels are $\\underline{not}$ independent: Maximum Entropy HMM\n",
    "- Markov Property: current state $s_t$ only depends on previous state $s_{t-1}$\n",
    "\n",
    "$\\textbf{Applications}$:\n",
    "\n",
    "A: transition probability matrix, B: observation probability matrix, $\\overrightarrow{\\pi}$: initial state\n",
    "- Given parameter $\\lambda = (A, B, \\overrightarrow{\\pi})$ and observation $O = (o_1, o_2, ..., o_T)$, compute $P(O; \\lambda)$\n",
    "  - Forward-backward algorithm\n",
    "- Given $O = (o_1, o_2, ..., o_T)$, estimate $\\lambda = (A, B, \\overrightarrow{\\pi})$ to maximize $P(O; \\lambda)$\n",
    "  - Supervised learning: maximal likelihoood estimation (MLE)\n",
    "  - Unsupervised learning: Baum-Welch algorithm (EM)\n",
    "- Given $\\lambda = (A, B, \\overrightarrow{\\pi})$ and $O = (o_1, o_2, ..., o_T)$, find sequence of hidden states $I = (i_1, i_2, ..., i_T)$ to maximize $P(I|O)$\n",
    "  - Vertibi algorithm (Dynamic Programming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
