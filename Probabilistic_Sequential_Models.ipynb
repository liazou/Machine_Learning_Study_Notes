{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "$$p(y_k|x_1, ..., x_m) = p(y_k)p(x_1,...,x_m|y_k) = p(y_k) \\prod_{i=1}^m p(x_i|y_k)$$\n",
    "\n",
    "$\\textbf{Steps}$:\n",
    "\n",
    "N: number of sample, m: number of features, K: number of labels\n",
    "1. Compute prior and conditional probability\n",
    "$$p(y=c_k) = \\frac{1}{N} \\sum_{i=1}^N 1(\\hat{y}_i = c_k)$$\n",
    "$$p(x_j=a_{j, l}| y = c_k) = \\frac{\\sum_{i=1}^N 1(x_{i,j}=a_{j,l}, \\hat{y}_i = c_k)}{\\sum_{i=1}^N 1(\\hat{y}_i = c_k)}$$\n",
    "$$j=1,2,....,m;l=1,2,...,s_j;k=1,2,...,K$$\n",
    "2. Given $\\overrightarrow{x}$, compute\n",
    "$$p(y_k|x_1, ..., x_m) = p(y_k=c_k) \\prod_{i=1}^m p(x_i|y_k=c_k)$$\n",
    "3. Predict \n",
    "$$\\hat{y} = \\underset{c_k}{\\mathrm{argmax}} [p(y_k=c_k) \\prod_{i=1}^m p(x_i|y_k=c_k)]$$\n",
    "\n",
    "$\\textbf{Assumptions}$:\n",
    "- Features are conditoinally independent\n",
    "\n",
    "$\\textbf{Advantages}$:\n",
    "- Fase for training and prediction\n",
    "- Easy to interpret (white box)\n",
    "- Few tunable hyperparameters\n",
    "- Less complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models (HMM)\n",
    "\n",
    "$\\textbf{Assumptions}$:\n",
    "- Independence: labels are independent\n",
    "  - What if labels are $\\underline{not}$ independent: Maximum Entropy Markove Model, Conditional Random Field\n",
    "- Markov Property: current state $s_t$ only depends on previous state $s_{t-1}$\n",
    "\n",
    "$\\textbf{Applications}$:\n",
    "\n",
    "A: transition probability matrix, B: observation probability matrix, $\\overrightarrow{\\pi}$: initial state\n",
    "- Given parameter $\\lambda = (A, B, \\overrightarrow{\\pi})$ and observation $O = (o_1, o_2, ..., o_T)$, compute $P(O; \\lambda)$\n",
    "  - Forward-backward algorithm\n",
    "    - Forward: $\\alpha_t(i) = P(o_1, o_2, ..., o_t, i_t = i; \\lambda)$\n",
    "      1. Compute $\\alpha_1(i) = \\pi_i b_i (o_1), i = 1, 2, ..., Q$\n",
    "      2. For $t= 1, 2, ..., T-1$ \n",
    "      $$\\alpha_{t+1}(i) = \\left[\\sum_{j=1}^Q \\alpha_t (i) a_{j,i}\\right] b_i (o_{t+1})$$\n",
    "      3. Stop: $P(O; \\lambda) = \\sum_{i=1}^Q \\alpha_T (i)$\n",
    "    - Backward: $\\beta_t (i) = P(o_{t+1}, o_{t+2}, ..., o_T | i_t = i; \\lambda)$\n",
    "      1. Initialize $\\beta_T (i) = 1, i = 1, 2, ..., Q$\n",
    "      2. For $t= T-1, T-2, ..., 1$\n",
    "      $$\\beta_t (i) = \\sum_{j=1}^Q a_{i,j} b_j (o_{t+1}) \\beta_{t+1} (i)$$\n",
    "      3. Step: $P(O;\\lambda) = \\sum_{i=1}^Q \\pi_i b_i (o_1) \\beta_1 (i)$\n",
    "      \n",
    "- Given $O = (o_1, o_2, ..., o_T)$, estimate $\\lambda = (A, B, \\overrightarrow{\\pi})$ to maximize $P(O; \\lambda)$\n",
    "  - Supervised learning: maximal likelihoood estimation (MLE)\n",
    "  - Unsupervised learning: Baum-Welch algorithm (EM)\n",
    "  \n",
    "- Given $\\lambda = (A, B, \\overrightarrow{\\pi})$ and $O = (o_1, o_2, ..., o_T)$, find sequence of hidden states $I = (i_1, i_2, ..., i_T)$ to maximize $P(I|O)$\n",
    "  - Vertibi algorithm (Dynamic Programming)\n",
    "    1. Initialize $\\delta_1 (i) = \\pi_i b_i (o_1), i = 1, 2, ..., Q$\n",
    "    2. For $t = 2, ..., T$\n",
    "    $$\\delta_t (i) = \\underset{1 \\leq j \\leq Q}{\\mathrm{max}} \\delta_{t-1}(i) a_{j,i} b_i (o_t)$$\n",
    "    $$\\Psi_t (i) = \\underset{1 \\leq j \\leq Q}{\\mathrm{argmax}} \\delta_{t-1}(i) a_{j,i}$$\n",
    "    3. Stop: $P^{*} = \\underset{1 \\leq j \\leq Q}{\\mathrm{max}} \\delta_T (i), i_T^{*} = \\underset{1 \\leq j \\leq Q}{\\mathrm{argmax}} \\delta_T (i)$\n",
    "    4. Backtracking: $i_t^{*} = \\Psi_{t+1} (i_{t+1}^{*})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Random Field (CRF)\n",
    "\n",
    "$\\textbf{Input}$:\n",
    "- Input vectors $X$\n",
    "- The position i of data point\n",
    "- The label of data point $i - 1$ in $X$: $y_{i-1}$\n",
    "- The label of data point $i$ in $X$: $y_{i}$\n",
    "\n",
    "$\\textbf{Objective}$:\n",
    "- Model conditional probability\n",
    "$$\\hat{y} = \\underset{y}{\\mathrm{argmax}}  p(y|X)$$\n",
    "- Does not require label independence\n",
    "\n",
    "$\\textbf{Feature Function}$:\n",
    "- Purpose: express the characteristic of data sequence\n",
    "- Example: Part-of-Speech tagging\n",
    "- Each feature function is based on label of previous word and current word\n",
    "$$f(X, i, L_{i-1}, L_{i})=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ L_{i-1} \\in Noun \\& L_{i} \\in Verb \\\\\n",
    "      0, & \\text{else}\\ \n",
    "    \\end{cases}$$\n",
    "- Assign each feature function with weights\n",
    "$$p(y, X, \\lambda) = \\frac{1}{Z(X)} exp\\left(\\sum_{i=1}^n \\sum_j \\lambda_j f_i (X, i, y_{i-1}, y_i)\\right)$$\n",
    "$$Z(x) = \\sum_{y^{'} \\in y} \\sum_{i=1}^n \\sum_j \\lambda_j f_i (X, i, y_{i-1}^{'}, y_i^{'})$$\n",
    "- Take negative loglikelihood, compute partial derivative w.r.t. $\\lambda$\n",
    "- Gradient Descent update for CRF\n",
    "$$\\lambda := \\lambda + \\alpha \\left(\\sum_{k=1}^m F_j (y^k, x^k) + \\sum_{k=1}^m p(y|x^k, \\lambda) F_j (y, x^k) \\right)$$\n",
    "$$F_j (y, x) = \\sum_{i=1}^n f_i (X, i, y_{i-1}, y_i)$$\n",
    "\n",
    "HMM vs CRF: https://medium.com/ml2vec/overview-of-conditional-random-fields-68a2a20fa541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
