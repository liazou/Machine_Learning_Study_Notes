{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM\n",
    "\n",
    "- Goal: Find a vector to the hyperplane that maximizes the margin\n",
    "  - Hyperplane:\n",
    "    - Positive $H_{positive}: w x + b = 1$\n",
    "    - Negative $H_{negative}: w x + b = -1$\n",
    "  - Margin: $\\hat{\\gamma}$\n",
    "- Support Vectors: data points that are closest to the hyperplane and influence the position / orientation of the pyperplane\n",
    "- Loss function: Hinge Loss\n",
    "$$\\mathcal{L} = \\sum_{i=1}^n max(0, 1 - y_i (w x_i + b))$$\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/921/1*06GSco3ItM3gwW2scY6Tmg.png\" alt=\"SVM\"  style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Derivations}$:\n",
    "- Primal:\n",
    "$$\\underset{\\gamma}{\\mathrm{max}} \\frac{\\hat{\\gamma}}{||w||} s.t. y_i (w x_i + b) \\geq \\hat{\\gamma}$$\n",
    "- Rescale $\\hat{\\gamma}=1$, transform to convex optimization:\n",
    "$$\\underset{w, b}{\\mathrm{min}} \\frac{1}{2} ||w||^2 s.t. 1 -  y_i (w x_i + b) \\leq 0$$\n",
    "- Lagrange Multiplier:\n",
    "$$L_p (w, b, a_i) = \\frac{1}{2} ||w||^2 + \\sum_i a_i [1 -  y_i (w x_i + b)]$$\n",
    "$$\\frac{\\partial L}{\\partial w} = w - \\sum_i a_i y_i x_i = 0, \\frac{\\partial L}{\\partial b} = -\\sum_i a_i y_i = 0$$\n",
    "- Plug in $L_p$, becomes a dual problem:\n",
    "$$\\underset{\\gamma}{\\mathrm{min}} L_D (w, b, a_i) = \\underset{\\gamma}{\\mathrm{min}} \\left( \\frac{1}{2} \\sum_{i, j} a_i a_j y_i y_j \\langle x_i, x_j \\rangle - \\sum_i a_i \\right)$$\n",
    "$$ s.t. \\sum_i a_i y_i = 0, a_i \\geq 0$$\n",
    "  - transform the problem by computing the inner product\n",
    "  - can solve non-linear separable problem by kernel trick\n",
    "- Decision function:\n",
    "$$f(x) = sign \\left(\\sum_{i=1}^N a_i \\tilde{y}_i x_i x + b \\right)$$\n",
    "\n",
    "$\\textbf{Sequential Minimum Optimization (SMO)}$:\n",
    "- choose some pair $a_i, a_j$, keep other $a_k$'s fixed\n",
    "- optimize $L_D (w, b, a_i, a_j)$ w.r.t $a_i, a_j$\n",
    "- repeat until convergence\n",
    "\n",
    "$\\textbf{Equivalent Optimization Problem}$:\n",
    "- Hard Margin:\n",
    "$$\\underset{w, b}{\\mathrm{min}} \\frac{1}{2} ||w||^2 s.t. y_i (w x_i + b) \\geq 1$$\n",
    "- Soft Margin:\n",
    "$$\\underset{w, b}{\\mathrm{min}} \\left[\\frac{1}{2} ||w||^2 + C \\sum_{i=1}^n \\xi^{(i)} \\right] s.t. y_i (w x_i + b) \\geq 1 - \\xi^{(i)}$$\n",
    "$$\\underset{w, b}{\\mathrm{min}} \\left[\\frac{1}{n}\\sum_{i=1}^n max(0, 1 - y_i (w x_i + b)) + \\lambda ||w||^2 \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel SVM\n",
    "\n",
    "$\\textbf{Mercer's Theorem}$: If K is positive semi-definite and symmetric, then exists a kernal $\\phi$ such that $K(a, b) = \\phi (a)^T \\phi (b)$\n",
    "\n",
    "$\\textbf{Kernel trick}$: \n",
    "- map the original input to a new feature space\n",
    "- replace inner product $ x_i \\cdot x_j $ by kernel $ \\phi (x_i) \\cdot \\phi (x_j) $\n",
    "- train linear SVM in a new feature space\n",
    "$$L(w, b, a_i) = \\frac{1}{2} \\sum_{i, j} a_i a_j y_i y_j \\langle x_i, x_j \\rangle - \\sum_i a_i$$\n",
    "- Decision function:\n",
    "$$f(x) = sign \\left(\\sum_{i=1}^N a_i \\tilde{y}_i K(x_i, x) + b \\right)$$\n",
    "\n",
    "$\\textbf{Different Types of Kernel}$:\n",
    "- linear: $K(a, b) = a^T b$\n",
    "- polynomial: $K(a, b) = (\\gamma a^T b + r)^d$\n",
    "- Gaussian rbf: $K(a, b) = exp \\left( \\frac{||a - b||^2}{2 \\sigma^2} \\right)$\n",
    "- sigmoid: $K(a, b) = tanh(\\gamma a^T b + r)$\n",
    "\n",
    "$\\textbf{Hyperparameters}$:\n",
    "- gamma $\\gamma$: increasing gamma leads to overfitting as the classifier tries to perfectly fit the training data\n",
    "- C (equivalent to $1 / \\lambda$): \n",
    "  - C is larger, the margin will be smaller\n",
    "  - C is smaller, the margin will be larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR)\n",
    "\n",
    "$\\textbf{Objective}$:\n",
    "$$\\underset{w, b}{\\mathrm{min}} \\left[\\frac{1}{2} ||w||^2 + C \\sum_{i=1}^n L_{\\epsilon} (w x_i + b - y_i) \\right]$$\n",
    "$$L_{\\epsilon} (z) = max(|z| - \\epsilon, 0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Domain Description (SVDD): One-class SVM\n",
    "\n",
    "$\\textbf{Objective}$: find a smallest sphere with center $o$, radius $R$ to include all positive samples\n",
    "$$\\underset{R, o, \\xi}{\\mathrm{min}} L(R, o, \\xi) = \\underset{R, o, \\xi}{\\mathrm{min}} \\left[ R^2 + C \\sum_{i=1}^n \\xi_i \\right]$$\n",
    "$$s.t. ||x_i 0 o||^2 \\leq R^2 + \\xi_i, \\xi_i \\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Advantages}$:\n",
    "- Fast prediction\n",
    "- Fit high-dimensional data\n",
    "- kernel function adapts to many types of data\n",
    "\n",
    "$\\textbf{Disadvantages}$:\n",
    "- Computational cost (with large training samples)\n",
    "- can only fit small sample size (smaller than 1 million)\n",
    "- Strongly dependent on softening parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machines (FM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Way FM\n",
    "\n",
    "$$\\hat{y} (x) = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n \\langle v_i, v_j \\rangle x_i x_j$$\n",
    "$$\\sum_{i=1}^n \\sum_{j=i+1}^n \\langle v_i, v_j \\rangle x_i x_j = \\frac{1}{2} \\sum_{f=1}^k \\left(\\left(\\sum_i^n v_{i, f} x_i \\right)^2 - \\sum_{i=1}^n v_{i,f}^2 x_i^2 \\right)$$\n",
    "$$Y = X \\beta + X W X^T = X \\beta + X V V^T X^T =  X \\beta + X V (XV)^T, W^{n*n} = V^{n*k} V'^{k*n} $$\n",
    "\n",
    "- $w_0$: global bias\n",
    "- $w_i$: weights of i-th variable\n",
    "- $\\langle v_i, v_j \\rangle$: intersection between i-th and j-th variable\n",
    "- optimization: stochastic gradient descent, alternative least square\n",
    "- mainly used for predicting missing scores in user-item matrix (handle sparse data well)\n",
    "\n",
    "Supervised tasks:\n",
    "- regression: $MSE = \\frac{1}{N} (y-\\hat{y}(x))^2$\n",
    "- classification $logloss = ln(e^{y*\\hat{y}(x)} + 1)$\n",
    "\n",
    "## Learning FM: Gradient Descent\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta} \\hat{y} (x) = \\begin{cases}\n",
    "      1, & \\text{if}\\ \\theta = w_0\\\\\n",
    "      x_i, & \\text{if}\\ \\theta = w_i\\\\\n",
    "      x_i \\sum_{j=1}^n v_{j, f} x_j - v_{i, f} x_i^2, & \\text{if}\\ \\theta = v_{i, f}\\\\\n",
    "    \\end{cases}$$\n",
    "    \n",
    "- $\\sum_{j=1}^n v_{i, f} x_j$ is indep of $i$ (can be precomputed)\n",
    "- Time complexicity: $O(kn)$ or $O(k m(x))$ (under sparsity)\n",
    "\n",
    "## d-way FM\n",
    "\n",
    "$$\\hat{y} (x) = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{l=2}^d \\sum_{i_1 = 1}^n \\ldots \\sum_{i_l = i_{l-1}+1}^n \\left( \\prod_{j=1}^l x_{i_j} \\right) \\left( \\sum_{f=1}^{k_l} \\prod_{j=1}^l v_{i_j, f}^{(l)} \\right)$$\n",
    "\n",
    "## Comparison with SVM\n",
    "\n",
    "Similarities:\n",
    "- linear SVM is identical FM of degree $d=1$\n",
    "- both models nest interactions up to degree $d=2$\n",
    "\n",
    "Differences:\n",
    "- all interaction $w_{i, j}$ of SVM are indep, where $\\langle v_i, v_j \\rangle$ depend on each other in FMs\n",
    "- model equation of FMs is indep of training data, SVMs prediction depends on support vectors\n",
    "- FMs can be learned in primal (gradient descent), non-linear SVMs are learned in dual\n",
    "- linear and polynomial SVMs do not work well in sparse data (e.g. user-item collaborative filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Factorization Machines: https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
