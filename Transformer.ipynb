{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "\n",
    "\n",
    "## Single-Head Attention\n",
    "\n",
    "$$Attention(Q, K, V) = softmax \\left( \\frac{Q K^T}{\\sqrt{d}} \\right) V$$\n",
    "\n",
    "- machine translation\n",
    "- $\\sqrt{d}$: scale down to avoid gradient vanishing\n",
    "\n",
    "$\\textbf{Key Components}$:\n",
    "- Q (to match others): vector representing the word\n",
    "- K (to be matched): memoery (all the words generated before)\n",
    "- V (info to be extracted): could be the same for key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/900/1*NIxhlMqHFyhllBm4v1j2-A.png\" alt=\"attention\" style=\"width:600px;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Steps}$:\n",
    "1. Compute similarity between query and source\n",
    "  - $e^{\\langle q, k \\rangle}$: similarity between query and key\n",
    "    - dot product: $Sim(Query, Key_i) = Query \\cdot Key_i$\n",
    "    - cosine similarity: $Sim(Query, Key_i) = \\frac{Query \\cdot Key_i}{||Query|| * ||Key||}$\n",
    "    - multi-layer preceptron: $Sim(Query, Key_i) = Sim(Query, Key_i)$\n",
    "2. softmax on all the similarities\n",
    "  - compute attention weights to each word (total weights equal to 1)\n",
    "  $$\\alpha^{\\langle q, k_i \\rangle} = softmax(e^{\\langle q, k_i \\rangle}) = \\frac{exp(e^{\\langle q, k_i \\rangle})}{\\sum_{j=1}^{L_x} exp(e^{\\langle q, k_j \\rangle})}$$\n",
    "3. compute attention between query and source\n",
    "  $$Attention(Query, Source) = \\sum_{i=1}^{L_x} \\alpha^{\\langle q, k_i \\rangle} * Value_i$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "$\\textbf{Steps}$:\n",
    "1. linear transformation on Q, K, V before attention\n",
    "  - W matrix: change dimension on each matrix from d to d'\n",
    "  $$\\tilde{V}_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "2. Concatenate (merge by column) all attentions\n",
    "  $$MultiHead(Q, K, V) = Concat(\\tilde{V}_1, ...,\\tilde{V}_a) W^O$$\n",
    "  \n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer.png\" alt=\"multi-head attention\" style=\"width:800px;\"/>\n",
    "\n",
    "$\\textbf{Different types of attention}$\n",
    "- Encoder-Decoder attention\n",
    "  - Query: previous decoder\n",
    "  - Source: current encoder\n",
    "  - Example: T5\n",
    "- Encoder self-attention\n",
    "  - Query and Source: previous encoder\n",
    "  - Example: BERT\n",
    "- Decoder masked self-attention\n",
    "  - Query and Source: previous decoder\n",
    "  - Example: GPT\n",
    "\n",
    "| Architecture       | Pros                                         | Cons                                         |\n",
    "|-------------------|--------------------------------------------|---------------------------------------------|\n",
    "| **Encoder-Decoder (Seq2Seq)** | ✅ Great for sequence-to-sequence tasks (translation, summarization)  <br> ✅ Contextualized bidirectional understanding <br> ✅ More flexible for structured output generation | ❌ Computationally expensive (both encoder & decoder) <br> ❌ Slower inference than decoder-only models |\n",
    "| **Encoder-Only**  | ✅ Deep bidirectional context understanding <br> ✅ Efficient for classification & retrieval tasks <br> ✅ Supports parallel processing | ❌ Cannot generate text <br> ❌ Limited to discriminative tasks |\n",
    "| **Decoder-Only**  | ✅ Highly efficient for text generation <br> ✅ Scales well for large datasets <br> ✅ Works well for open-ended generation (chatbots, coding) | ❌ Lacks deep bidirectional context <br> ❌ Struggles with tasks requiring full-sentence understanding <br> ❌ Prone to hallucination |\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "- generate next words based on previous words\n",
    "- prevent models from referring future words\n",
    "\n",
    "$$\\text{masked attention(Q, K, V)} = softmax \\left(\\frac{Q K^T + M}{\\sqrt{d}} \\right)$$\n",
    "- M: mask matrix of 0 and $-\\infty$\n",
    "\n",
    "## Positional Embedding\n",
    "\n",
    "$$PE_{position, 2i} = \\sin \\left( \\frac{position}{10000^{2i/d}} \\right), PE_{position, 2i+1} = \\cos \\left( \\frac{position}{10000^{2i/d}} \\right)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Self-Attention and CNN/RNN\n",
    "- Self-Attention: can parallel (GPU), no position info\n",
    "- RNN: hard to parallel, large number of training steps\n",
    "- CNN: can parallel, but hard to handle sequence data\n",
    "\n",
    "| Layer Type | Complexity per Layer | Sequential Operations | Max Path Length |\n",
    "| --- | --- | --- | --- |\n",
    "| Self-Attention | $O(n^2 * d)$ | $O(1)$ | $O(1)$ |\n",
    "| RNN | $O(n * d^2)$ | $O(n)$ | $O(n)$ |\n",
    "| CNN | $O(k * n * d^2)$ | $O(1)$ | $O(\\log_k (n))$ |\n",
    "| Self-Attention (restricted) | $O(r * n * d)$ | $O(1)$ | $O(n / r)$ |\n",
    "\n",
    "- n = sequence length\n",
    "- d = representation dimension\n",
    "- k = kernel size of CNN\n",
    "- r = neighbor size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"middle\">\n",
    "  <img src=\"images/Transformer/Transformer.png\" width=\"350\" />\n",
    "  <img src=\"images/Transformer/Cross_attention.png\" width=\"350\" /> \n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "| Autoregressive Transformer | Non-autoregressive Transformer |\n",
    "| --- | --- |\n",
    "|![image.png](images/Transformer/AT_Decoder.png)|![image.png](images/Transformer/NAT_Decoder.png)|\n",
    "| Output word one after the other | Output the entire sequence |\n",
    "| Advantage: better performance than NAT | Advantage: parallel, controllable output length |\n",
    "| Disadvantage: cannot parallel | Disadvantage: multi-modality |\n",
    "\n",
    "Transformer Slides: \n",
    "- 2019: https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf\n",
    "- 2021: https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/seq2seq_v9.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning (Pretrain + Fine-tune)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "- architecture: bidirectional self-attention transformer (encoder transformer)\n",
    "- activation function: Gaussian Error Linear Unit\n",
    "  $$GELU(x) = 0.5 x \\left(1 + tanh \\left(\\frac{\\pi}{2} (x + 0.044715 x^3) \\right) \\right)$$\n",
    "  \n",
    "<img src=\"https://pytorch.org/assets/images/bert1.png\" alt=\"BERT\" style=\"width:800px;\"/>\n",
    "\n",
    "$\\textbf{Masked Language Model (MLM)}$\n",
    "- the man went to [mask1], he bought a [mask2] of milk\n",
    "- $\\hat{x}$: masked sequence\n",
    "$$p(x_{1:T}) \\approx \\prod_{t=1}^T p(x_t | \\hat{x})^{\\textbf{1}(token is maksed)}$$\n",
    "\n",
    "$\\textbf{Next Sentence Prediction (NSP)}$\n",
    "- binary classification\n",
    "- sentence relationship: \n",
    "  - Question Answering (QA)\n",
    "  - Natural Language Inference (NLI)\n",
    "- example:\n",
    "  - sentence1: the man went to the store\n",
    "  - sentence2: he bought a gallon of milk\n",
    "  - Label: IsNext (1)\n",
    "\n",
    "$$P_i^{start} = softmax(\\overrightarrow{S} \\cdot \\overrightarrow{T_i}), P_i^{end} = softmax(\\overrightarrow{E} \\cdot \\overrightarrow{T_i})$$\n",
    "- objective: maximize likelihood of start and end\n",
    "$$max \\sum_{(x_s, b_s, e_s) \\in D} \\left(log(P_{b_s}^{start}) + log(P_{e_s}^{end})\\right)$$\n",
    "\n",
    "$\\textbf{Token representation}$:\n",
    "- token embedding\n",
    "- segment embedding\n",
    "- position embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT\n",
    "- semi-supervised learning\n",
    "- unidirectional auto-regressive generative language model\n",
    "$$p(x_{1:T}) = \\prod_{t=1}^T p(x_t | x_{0:t-1})$$\n",
    "\n",
    "$\\textbf{Pretrain}$: using unsupervised learning (masked language model, MLM) to predict missing words\n",
    "- covers all layers in model: embedding, transformer, output layer, etc.\n",
    "- why need unsupervised pre-training: help generalization (robustness) in deep learning\n",
    "- Reference: https://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf\n",
    "- multi-layer transformer decoder (unidirectional self-attention)\n",
    "- k: window size\n",
    "$$L_1(s) = \\sum_{t=1}^T log(p(w_t | w_{t-k}, ..., w_{t-1};\\Theta))$$\n",
    "\n",
    "$\\textbf{Fine-tune}$: supervised learning\n",
    "- only tune small portion of layers: usually final layer (depends on tasks)\n",
    "- tasks: natural language inference, question answering, sentence similarity, classification\n",
    "- $h^{(i)}$: output of final layer transformer\n",
    "$$L_2(s^{(i)}, y^{(i)}) = log(p(y^{(i)}|s^{(i)}))$$\n",
    "$$p(y^{(i)}|s^{(i)}) = softmax(W_{y^{(i)}}^T h^{(i)})$$\n",
    "\n",
    "$\\textbf{Loss function}$:\n",
    "$$L_3(s, y) = L_2(s^{(i)}, y^{(i)}) + \\lambda L_1(s)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet\n",
    "- permuted language model\n",
    "\n",
    "$\\textbf{Loss function}$:\n",
    "- log probabilities of all permutations\n",
    "- $Z_T$: all permutations\n",
    "$$\\underset{\\theta}{\\mathrm{max}} E_{z \\in Z_T} \\left(\\sum_{t=1}^T log(p_{\\theta}(x_t | x_{1:t-1})) \\right)$$\n",
    "\n",
    "$\\textbf{architecture}$: two-stream self-attention\n",
    "- query stream\n",
    "- content stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning\n",
    "\n",
    "| Fine-Tuning Technique | Description | Pros | Cons | Use Cases |\n",
    "|----------------------|-------------|------|------|-----------|\n",
    "| **Full Fine-Tuning** | Updates all model weights | ✅ High performance <br> ✅ Best for domain-specific models | ❌ Expensive <br> ❌ Risk of catastrophic forgetting | Specialized chatbots, medical/legal AI |\n",
    "| **LoRA** | Adds trainable low-rank matrices | ✅ Memory-efficient <br> ✅ Works on large models | ❌ Less effective for extreme domain shifts | Domain adaptation, cost-effective fine-tuning |\n",
    "| **Adapters** | Inserts small trainable layers | ✅ Multi-task flexibility <br> ✅ Lower cost | ❌ Adds inference latency | Multi-task learning, quick adaptation |\n",
    "| **Prefix Tuning** | Optimizes a small set of continuous vectors | ✅ Lightweight <br> ✅ No need to modify model weights | ❌ Limited adaptation | Task-specific tuning with minimal resources |\n",
    "| **Prompt Tuning** | Learns continuous prompt embeddings | ✅ Extremely parameter-efficient | ❌ Works best with instruction-tuned models | Efficient tuning of API-based LLMs |\n",
    "| **RLHF** | Fine-tunes based on human feedback | ✅ Improves safety & alignment | ❌ Expensive <br> ❌ Requires human labeling | Chatbots, AI alignment |\n",
    "| **RLAIF** | Fine-tunes using AI feedback | ✅ Scalable alternative to RLHF | ❌ Reinforces model biases | Large-scale alignment |\n",
    "| **Multi-Task Fine-Tuning** | Trains on multiple tasks simultaneously | ✅ Efficient for general-purpose AI | ❌ Risk of task interference | General NLP assistants |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Attention: https://arxiv.org/pdf/1706.03762.pdf\n",
    "- BERT: https://arxiv.org/pdf/1810.04805.pdf\n",
    "- XLNet: https://arxiv.org/pdf/1906.08237.pdf\n",
    "- GPT: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
