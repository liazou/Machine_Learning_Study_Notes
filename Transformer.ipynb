{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "## Single-Head Attention\n",
    "\n",
    "$$Attention(Q, K, V) = softmax \\left( \\frac{Q K^T}{\\sqrt{d}} \\right) V$$\n",
    "- machine translation\n",
    "- $\\sqrt{d}$: scale down to avoid gradient vanishing\n",
    "\n",
    "$\\textbf{Key Components}$:\n",
    "- Q (query for decoder): vector representing the word\n",
    "Source for encoder:\n",
    "- K (key): memoery (all the words generated before)\n",
    "- V (value): could be the same for key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/900/1*NIxhlMqHFyhllBm4v1j2-A.png\" alt=\"attention\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Steps}$:\n",
    "1. Compute similarity between query and source\n",
    "  - $e^{\\langle q, k \\rangle}$: similarity between query and key\n",
    "    - dot product: $Sim(Query, Key_i) = Query \\cdot Key_i$\n",
    "    - cosine similarity: $Sim(Query, Key_i) = \\frac{Query \\cdot Key_i}{||Query|| * ||Key||}$\n",
    "    - multi-layer preceptron: $Sim(Query, Key_i) = Sim(Query, Key_i)$\n",
    "2. softmax on all the similarities\n",
    "  - compute attention weights to each word (total weights equal to 1)\n",
    "  $$\\alpha^{\\langle q, k_i \\rangle} = softmax(e^{\\langle q, k_i \\rangle}) = \\frac{exp(e^{\\langle q, k_i \\rangle})}{\\sum_{j=1}^{L_x} exp(e^{\\langle q, k_j \\rangle})}$$\n",
    "3. compute attention etween query and source\n",
    "  $$Attention(Query, Source) = \\sum_{i=1}^{L_x} \\alpha^{\\langle q, k_i \\rangle} * Value_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "$\\textbf{Steps}$:\n",
    "1. linear transformation on Q, K, V before attention\n",
    "  - W matrix: change dimension on each matrix from d to d'\n",
    "  $$\\tilde{V}_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "2. Concatenate (merge by column) all attentions\n",
    "  $$MultiHead(Q, K, V) = Concat(\\tilde{V}_1, ...,\\tilde{V}_a) W^O$$\n",
    "  \n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer.png\" alt=\"multi-head attention\" style=\"width:800px;\"/>\n",
    "\n",
    "$\\textbf{Different types of attention}$\n",
    "- Encoder-Decoder attention\n",
    "  - Query: previous decoder\n",
    "  - Source: current encoder\n",
    "- Encoder self-attention\n",
    "  - Query and Source: previous encoder\n",
    "- Decoder masked self-attention\n",
    "  - Query and Source: previous decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning (Pretrain + Fine-tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "- architecture: bidirectional self-attention transformer (encoder transformer)\n",
    "- activation function: Gaussian Error Linear Unit\n",
    "  $$GELU(x) = 0.5 x \\left(1 + tanh \\left(\\frac{\\pi}{2} (x + 0.044715 x^3) \\right) \\right)$$\n",
    "  \n",
    "<img src=\"https://pytorch.org/assets/images/bert1.png\" alt=\"BERT\" style=\"width:800px;\"/>\n",
    "\n",
    "$\\textbf{Masked Language Model (MLM)}$\n",
    "- the man went to [mask1], he bought a [mask2] of milk\n",
    "- $\\hat{x}$: masked sequence\n",
    "$$p(x_{1:T}) \\approx \\prod_{t=1}^T p(x_t | \\hat{x})^{\\textbf{1}(token is maksed)}$$\n",
    "\n",
    "$\\textbf{Next Sentence Prediction (NSP)}$\n",
    "- binary classification\n",
    "- sentence relationship: \n",
    "  - Question Answering (QA)\n",
    "  - Natural Language Inference (NLI)\n",
    "- example:\n",
    "  - sentence1: the man went to the store\n",
    "  - sentence2: he bought a gallon of milk\n",
    "  - Label: IsNext (1)\n",
    "\n",
    "$$P_i^{start} = softmax(\\overrightarrow{S} \\cdot \\overrightarrow{T_i}), P_i^{end} = softmax(\\overrightarrow{E} \\cdot \\overrightarrow{T_i})$$\n",
    "- objective: maximize likelihood of start and end\n",
    "$$max \\sum_{(x_s, b_s, e_s) \\in D} \\left(log(P_{b_s}^{start}) + log(P_{e_s}^{end})\\right)$$\n",
    "\n",
    "$\\textbf{Token representation}$:\n",
    "- token embedding\n",
    "- segment embedding\n",
    "- position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT\n",
    "- semi-supervised learning\n",
    "- auto-regressive generative language model\n",
    "$$p(x_{1:T}) = \\prod_{t=1}^T p(x_t | x_{0:t-1})$$\n",
    "\n",
    "$\\textbf{Pretrain}$: unsupervised learning\n",
    "- multi-layer transformer decoder (unidirectional self-aatention)\n",
    "- k: window size\n",
    "$$L_1(s) = \\sum_{t=1}^T log(p(w_t | w_{t-k}, ..., w_{t-1};\\Theta))$$\n",
    "\n",
    "$\\textbf{Fine-tune}$: supervised learning\n",
    "- depends on tasks\n",
    "- $h^{(i)}$: output of final layer transformer\n",
    "$$L_2(s^{(i)}, y^{(i)}) = log(p(y^{(i)}|s^{(i)}))$$\n",
    "$$p(y^{(i)}|s^{(i)}) = softmax(W_{y^{(i)}}^T h^{(i)})$$\n",
    "\n",
    "$\\textbf{Loss function}$:\n",
    "$$L_3(s, y) = L_2(s^{(i)}, y^{(i)}) + \\lambda L_1(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet\n",
    "- permuted language model\n",
    "\n",
    "$\\textbf{Loss function}$:\n",
    "- log probabilities of all permutations\n",
    "- $Z_T$: all permutations\n",
    "$$\\underset{\\theta}{\\mathrm{max}} E_{z \\in Z_T} \\left(\\sum_{t=1}^T log(p_{\\theta}(x_t | x_{1:t-1})) \\right)$$\n",
    "\n",
    "$\\textbf{architecture}$: two-stream self-attention\n",
    "- query stream\n",
    "- content stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Attention: https://arxiv.org/pdf/1706.03762.pdf\n",
    "- BERT: https://arxiv.org/pdf/1810.04805.pdf\n",
    "- XLNet: https://arxiv.org/pdf/1906.08237.pdf\n",
    "- GPT: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
