{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "$$\\hat{\\beta} = \\frac{Cov(x, y)}{Var(x)} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "$\\textbf{Derivations}$:\n",
    "$$\\mathcal{L} = (Y - X \\beta)^T (Y - X \\beta) = \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = 2 * X^T (Y - X \\beta) = 0$$\n",
    "\n",
    "$$\\beta = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Assumptions}$:\n",
    "\n",
    "- Linearity between DV (y) and IV (X)\n",
    "  - scatterplot\n",
    "- Normality of residuals\n",
    "  - Shapir-Wilk test\n",
    "  - Kolmogorovâ€“Smirnov test\n",
    "  - QQ plot: compare quantles of data and normality\n",
    "  - $\\underline{Alternative}$: Use MAE when assume Laplace Distribution\n",
    "  $$pdf: f(x) = \\frac{1}{2 b} exp \\{ -\\frac{|x - \\mu|}{2 b} \\}$$\n",
    "  - Why if normality is not met: Maximum Likelihood (MLE) is not equivalent to Least Square (OLS)\n",
    "- Homoscedasticity (equal variance) of residuals\n",
    "  - Breusch Pagan Test\n",
    "    - obtain squared residual $\\hat{u}^2$ from OLS\n",
    "    - regress $\\hat{u}^2$ on all independent variables ($x_1, x_2, ..., x_k$)\n",
    "    - get $R_{\\hat{u}^2}^2$\n",
    "    - compute F statistic and p-value\n",
    "  - Scatterplot of residual vs predictor\n",
    "  - How to deal with heteroscedasticity:\n",
    "    - obtain residual $\\hat{u}$ from OLS\n",
    "    - regress $ln(\\hat{u}^2)$ on $x_1, x_2, ..., x_k$\n",
    "    - exponentiate the fitted values $\\hat{h} = exp(\\hat{g})$\n",
    "    - run WLS with weights $1 / \\hat{h}$\n",
    "- No multicollinearity of independent variables\n",
    "  - How to deal with multicollinearity:\n",
    "    - Regularization\n",
    "    - PCA\n",
    "    - VIF\n",
    "- How to compute VIF\n",
    "  - regress k-th variables on other independent variables\n",
    "  $$VIF = \\frac{1}{1 - R_k^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Goodness of Fit}$:\n",
    "\n",
    "$$SST = \\sum_i (y_i - \\bar{y})^2, SSE = \\sum_i (y_i - \\hat{y_i})^2, SSR = \\sum_i (\\hat{y_i} - \\bar{y})^2 $$\n",
    "$$SST = SSE + SSR$$ \n",
    "$$R^2 = 1 - \\frac{SSE}{SST} = \\frac{SSR}{SST} = \\frac{[Cov(x, y)]^2}{Var(x) Var(y)}$$\n",
    "$$Adjusted R^2 = 1 - \\frac{(1 - R^2) * (N - 1)}{N - k - 1} < R^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Linear Regression Questions}$:\n",
    "- Regress y on x, regress x on y, what's relationship between $\\hat{\\beta}_{y|x}$ and $\\hat{\\beta}_{x|y}$\n",
    "  - $\\hat{\\beta}_{y|x} = \\frac{Cov(x, y)}{Var(x)}, \\hat{\\beta}_{x|y} = \\frac{Cov(x, y)}{Var(y)}, \\hat{\\beta}_{y|x} * \\hat{\\beta}_{x|y} = \\frac{[Cov(x, y)]^2}{Var(x) Var(y)} = R^2$\n",
    "- Duplicate data, how will coefficient, $R^2$, standard error/variance ($Var(\\hat{\\beta})$) change\n",
    "  - $\\textbf{coefficient}$: same\n",
    "    - $MSE = \\frac{1}{2n}\\sum_{i=1}^{2n} (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 = \\frac{2}{2n}\\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2$\n",
    "    - same loss function, get same coefficient\n",
    "  - $R^2$: same\n",
    "    - $R^2 = \\frac{[Cov(x, y)]^2}{Var(x) Var(y)}$\n",
    "    - numerator and denominator cancel\n",
    "  - $\\textbf{standard error/variance}$: smaller\n",
    "- Univariate regression significant, multivariate regression not significant\n",
    "  - Exist multicollinearity\n",
    "- How does multicollinearity affect standard error/variance, t-statistic, p-value\n",
    "  - $\\textbf{standard error/variance}$: larger\n",
    "    - multicollinearity: X is not full ranked, $(X^T X)^{-1}$ unstable\n",
    "  - $\\textbf{t-statistic}$: smaller\n",
    "    - $t_{\\hat{\\beta}} = \\frac{\\hat{\\beta} - \\beta_0}{s.e.(\\hat{\\beta})}$\n",
    "  - $\\textbf{p-value}$: larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "n = 300\n",
    "x1 = np.random.normal(size=n)\n",
    "y1 = 2 * x1 + np.random.normal(size=n) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.815\n",
      "Model:                            OLS   Adj. R-squared:                  0.815\n",
      "Method:                 Least Squares   F-statistic:                     1320.\n",
      "Date:                Thu, 20 Aug 2020   Prob (F-statistic):          1.06e-111\n",
      "Time:                        23:37:50   Log-Likelihood:                -403.88\n",
      "No. Observations:                 300   AIC:                             809.8\n",
      "Df Residuals:                     299   BIC:                             813.5\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             1.9794      0.054     36.337      0.000       1.872       2.087\n",
      "==============================================================================\n",
      "Omnibus:                        0.956   Durbin-Watson:                   1.944\n",
      "Prob(Omnibus):                  0.620   Jarque-Bera (JB):                0.985\n",
      "Skew:                          -0.019   Prob(JB):                        0.611\n",
      "Kurtosis:                       2.722   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model1 = OLS(endog=y1, exog=x1).fit()\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.815\n",
      "Model:                            OLS   Adj. R-squared:                  0.815\n",
      "Method:                 Least Squares   F-statistic:                     2645.\n",
      "Date:                Thu, 20 Aug 2020   Prob (F-statistic):          6.66e-222\n",
      "Time:                        23:37:51   Log-Likelihood:                -807.75\n",
      "No. Observations:                 600   AIC:                             1618.\n",
      "Df Residuals:                     599   BIC:                             1622.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             1.9794      0.038     51.431      0.000       1.904       2.055\n",
      "==============================================================================\n",
      "Omnibus:                        2.265   Durbin-Watson:                   1.944\n",
      "Prob(Omnibus):                  0.322   Jarque-Bera (JB):                1.970\n",
      "Skew:                          -0.019   Prob(JB):                        0.373\n",
      "Kurtosis:                       2.722   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Duplicate data\n",
    "x1_dup = np.concatenate((x1, x1), axis=0)\n",
    "y1_dup = np.concatenate((y1, y1), axis=0)\n",
    "\n",
    "model1_dup = OLS(endog=y1_dup, exog=x1_dup).fit()\n",
    "print(model1_dup.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.009\n",
      "Model:                            OLS   Adj. R-squared:                  0.003\n",
      "Method:                 Least Squares   F-statistic:                     1.410\n",
      "Date:                Thu, 20 Aug 2020   Prob (F-statistic):              0.246\n",
      "Time:                        23:37:51   Log-Likelihood:                -436.07\n",
      "No. Observations:                 300   AIC:                             876.1\n",
      "Df Residuals:                     298   BIC:                             883.5\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.0962      0.059     -1.631      0.104      -0.212       0.020\n",
      "x2            -0.0170      0.063     -0.270      0.787      -0.141       0.107\n",
      "==============================================================================\n",
      "Omnibus:                        0.584   Durbin-Watson:                   2.096\n",
      "Prob(Omnibus):                  0.747   Jarque-Bera (JB):                0.448\n",
      "Skew:                           0.091   Prob(JB):                        0.799\n",
      "Kurtosis:                       3.054   Cond. No.                         1.11\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Multicollinearity\n",
    "x2 = np.random.normal(loc=x1, scale=0.01, size=n)\n",
    "y2 = x1 - x2 + np.random.normal(size=n)\n",
    "x1x2 = np.resize(np.array((x1, x2)), (n, 2))\n",
    "\n",
    "model2 = OLS(endog=y2, exog=x1x2).fit()\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression (L1)\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|$$\n",
    "$$ \\hat{\\beta}_j^{lasso} = sgn(\\beta_j^{OLS}) max(0, |\\beta_j^{OLS}| - \\gamma), \\gamma = \\frac{n \\lambda}{||x||^2}$$\n",
    "https://stats.stackexchange.com/questions/17781/derivation-of-closed-form-lasso-solution\n",
    "\n",
    "Usage:\n",
    "- variable selection\n",
    "- parameter shrinkage\n",
    "- penalize $\\beta_j$ to exactly zero\n",
    "\n",
    "## Ridge Regression (L2)\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2$$\n",
    "$$ \\hat{\\beta}_j^{ridge} = (X^T X + \\lambda I_p)^{-1} X^T y$$\n",
    "\n",
    "Usage:\n",
    "- parameter shrinkage\n",
    "- include all independent variables\n",
    "- penalize $\\beta_j$ close to zero\n",
    "\n",
    "## Elastic Net (L1 + L2)\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 + \\alpha \\lambda \\sum_{j=1}^p \\beta_j^2 + (1 - \\alpha) \\lambda \\sum_{j=1}^p |\\beta_j|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Regularization Questions}$:\n",
    "- Need standardization before regularization\n",
    "  - Why: Transform features to same scale\n",
    "- Why does regularization reduce overfitting\n",
    "  - Large $\\lambda$ penalizes parameters/weights (close) to zero\n",
    "  - Reduce Model complexity\n",
    "- Why do not regularize bias term ($\\beta_0$)\n",
    "  - Var(X + $\\beta_0$) = Var(X)\n",
    "  - Bias term does not increase variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Each observation: \n",
    "$$f(y_i) = p^{y_i} (1-p)^{1-{y_i}}$$\n",
    "\n",
    "Likelihood:\n",
    "$$L(p) = \\prod_i f(y_i) = p^{\\sum_i y_i} (1-p)^{\\sum_i (1 - y_i)}$$\n",
    "\n",
    "Log-likelihood:\n",
    "$$\\mathcal{l} = log(L(p)) = \\sum_i y_i log(p) + \\sum_i (1 - y_i) log(1-p)$$\n",
    "\n",
    "Loss function (Cross Entropy):\n",
    "$$\\mathcal{L} = -\\sum_i [y_i log(p_i) + (1 - y_i) log(1 - p_i)]$$\n",
    "\n",
    "$\\textbf{Assumptions}$:\n",
    "- linearity of indepdent variables and log odds\n",
    "$$p = \\frac{1}{1 + exp(-y)}, y = ln(\\frac{p}{1-p}) = X \\beta$$\n",
    "- every observation is independent Bernoulli(p)\n",
    "- no multicollinearity\n",
    "\n",
    "$\\textbf{Pseudo R-Squared}$: https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/#:~:text=A%20pseudo%20R%2Dsquared%20only,model%20better%20predicts%20the%20outcome.\n",
    "$$R^2 = 1 - \\frac{\\sum_i (y_i - p_i)^2}{\\sum_i (y_i - \\bar{y})^2}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
