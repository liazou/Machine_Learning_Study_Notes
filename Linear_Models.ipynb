{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "$$\\hat{\\beta} = \\frac{Cov(x, y)}{Var(x)} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "$\\textbf{Derivations}$:\n",
    "$$\\mathcal{L} = (Y - X \\beta)^T (Y - X \\beta) = \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = 2 * X^T (Y - X \\beta) = 0$$\n",
    "\n",
    "$$\\beta = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Assumptions}$:\n",
    "\n",
    "- Linearity between DV (y) and IV (X)\n",
    "  - scatterplot\n",
    "- Normality of residuals\n",
    "  - Shapir-Wilk test\n",
    "  - Kolmogorovâ€“Smirnov test\n",
    "  - QQ plot: compare quantles of data and normality\n",
    "  - $\\underline{Alternative}$: Use MAE when assume Laplace Distribution\n",
    "  $$pdf: f(x) = \\frac{1}{2 b} exp \\left\\{ -\\frac{|x - \\mu|}{2 b} \\right\\}$$\n",
    "  - Why if normality is not met: Maximum Likelihood (MLE) is not equivalent to Least Square (OLS)\n",
    "- Homoscedasticity (equal variance) of residuals\n",
    "  - Breusch Pagan Test\n",
    "    - obtain squared residual $\\hat{u}^2$ from OLS\n",
    "    - regress $\\hat{u}^2$ on all independent variables ($x_1, x_2, ..., x_k$)\n",
    "    - get $R_{\\hat{u}^2}^2$\n",
    "    - compute F statistic and p-value\n",
    "  - Scatterplot of residual vs predictor\n",
    "  - How to deal with heteroscedasticity:\n",
    "    - obtain residual $\\hat{u}$ from OLS\n",
    "    - regress $ln(\\hat{u}^2)$ on $x_1, x_2, ..., x_k$\n",
    "    - exponentiate the fitted values $\\hat{h} = exp(\\hat{g})$\n",
    "    - run WLS with weights $1 / \\hat{h}$\n",
    "- No multicollinearity of independent variables\n",
    "  - How to deal with multicollinearity:\n",
    "    - Regularization\n",
    "    - PCA\n",
    "    - VIF\n",
    "- How to compute VIF\n",
    "  - regress k-th variables on other independent variables\n",
    "  $$VIF = \\frac{1}{1 - R_k^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Goodness of Fit}$:\n",
    "\n",
    "$$SST = \\sum_i (y_i - \\bar{y})^2, SSE = \\sum_i (y_i - \\hat{y_i})^2, SSR = \\sum_i (\\hat{y_i} - \\bar{y})^2 $$\n",
    "$$SST = SSE + SSR$$ \n",
    "$$R^2 = 1 - \\frac{SSE}{SST} = \\frac{SSR}{SST} = \\frac{[Cov(x, y)]^2}{Var(x) Var(y)}$$\n",
    "$$Adjusted R^2 = 1 - \\frac{(1 - R^2) * (N - 1)}{N - k - 1} < R^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Linear Regression Questions}$:\n",
    "- Regress y on $x_1$ and $x_2$, regress y on $x_1 + x_2$ and $x_1 - x_2$\n",
    "  - What's the difference of coefficients?\n",
    "  $$y = X \\beta, y = \\tilde{X} \\tilde{\\beta}, \\tilde{X} = XA, X = [x_1, x_2], \n",
    "  A = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}$$\n",
    "  $$\\beta = (X^T X)^{-1} X^T y, \\tilde{\\beta} = (\\tilde{X}^T \\tilde{X})^{-1} \\tilde{X}^T y = ((XA)^T XA)^{-1} (XA)^T y$$\n",
    "  $$ = (A^T X^T X A)^{-1} A^T X^T y = A^{-1} (X^T X)^{-1} (A^T)^{-1} A^T X^T y = A^{-1} (X^T X)^{-1}X^T y = A^{-1} \\beta$$\n",
    "  - Is there a difference of prediction on training and testing data? (No)\n",
    "  $$\\hat{y} = X \\beta, \\hat{y} = \\tilde{X} \\tilde{\\beta} = X A A^{-1} \\beta = X \\beta$$\n",
    "  - Will it change if the model is regularized? (Yes)\n",
    "  $$\\beta = (X^T X + \\lambda W)^{-1} X^T y, \\tilde{\\beta} = (\\tilde{X}^T \\tilde{X} + \\lambda W)^{-1} \\tilde{X}^T y$$\n",
    "  $$ = ((XA)^T XA + \\lambda W)^{-1} (XA)^T y = (A^T X^T XA + A^T \\frac{\\lambda}{2} W A)^{-1} A^T X^T y$$\n",
    "  $$ = (A^T(X^T X + \\frac{\\lambda}{2} W) A)^{-1} A^T X^T y = A^{-1} (X^T X + \\frac{\\lambda}{2} W)^{-1} (A^T)^{-1} A^T X^T y$$\n",
    "  $$ = A^{-1} (X^T X + \\frac{\\lambda}{2} W)^{-1} X^T y$$\n",
    "- Regress y on x, regress x on y, what's relationship between $\\hat{\\beta}_{y|x}$ and $\\hat{\\beta}_{x|y}$\n",
    "  - $\\hat{\\beta}_{y|x} = \\frac{Cov(x, y)}{Var(x)}, \\hat{\\beta}_{x|y} = \\frac{Cov(x, y)}{Var(y)}, \\hat{\\beta}_{y|x} * \\hat{\\beta}_{x|y} = \\frac{[Cov(x, y)]^2}{Var(x) Var(y)} = R^2$\n",
    "- Duplicate data, how will coefficient, $R^2$, standard error/variance ($Var(\\hat{\\beta})$) change\n",
    "  - $\\textbf{coefficient}$: same\n",
    "    - $MSE = \\frac{1}{2n}\\sum_{i=1}^{2n} (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 = \\frac{2}{2n}\\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2$\n",
    "    - same loss function, get same coefficient\n",
    "  - $R^2$: same\n",
    "    - $R^2 = \\frac{[Cov(x, y)]^2}{Var(x) Var(y)}$\n",
    "    - numerator and denominator cancel\n",
    "  - $\\textbf{standard error/variance}$: smaller\n",
    "- Univariate regression significant, multivariate regression not significant\n",
    "  - Exist multicollinearity\n",
    "- How does multicollinearity affect standard error/variance, t-statistic, p-value\n",
    "  - $\\textbf{standard error/variance}$: larger\n",
    "    - multicollinearity: X is not full ranked, $(X^T X)^{-1}$ unstable\n",
    "  - $\\textbf{t-statistic}$: smaller\n",
    "    - $t_{\\hat{\\beta}} = \\frac{\\hat{\\beta} - \\beta_0}{s.e.(\\hat{\\beta})}$\n",
    "  - $\\textbf{p-value}$: larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "n = 300\n",
    "x1 = np.random.normal(size=n)\n",
    "y1 = 2 * x1 + np.random.normal(size=n) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.825\n",
      "Model:                            OLS   Adj. R-squared:                  0.825\n",
      "Method:                 Least Squares   F-statistic:                     1412.\n",
      "Date:                Sun, 27 Sep 2020   Prob (F-statistic):          2.73e-115\n",
      "Time:                        21:42:37   Log-Likelihood:                -411.30\n",
      "No. Observations:                 300   AIC:                             824.6\n",
      "Df Residuals:                     299   BIC:                             828.3\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             2.0348      0.054     37.580      0.000       1.928       2.141\n",
      "==============================================================================\n",
      "Omnibus:                        0.519   Durbin-Watson:                   1.955\n",
      "Prob(Omnibus):                  0.771   Jarque-Bera (JB):                0.591\n",
      "Skew:                          -0.097   Prob(JB):                        0.744\n",
      "Kurtosis:                       2.900   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model1 = OLS(endog=y1, exog=x1).fit()\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.825\n",
      "Model:                            OLS   Adj. R-squared:                  0.825\n",
      "Method:                 Least Squares   F-statistic:                     2829.\n",
      "Date:                Sun, 27 Sep 2020   Prob (F-statistic):          4.36e-229\n",
      "Time:                        21:42:37   Log-Likelihood:                -822.59\n",
      "No. Observations:                 600   AIC:                             1647.\n",
      "Df Residuals:                     599   BIC:                             1652.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             2.0348      0.038     53.191      0.000       1.960       2.110\n",
      "==============================================================================\n",
      "Omnibus:                        1.102   Durbin-Watson:                   1.956\n",
      "Prob(Omnibus):                  0.576   Jarque-Bera (JB):                1.183\n",
      "Skew:                          -0.097   Prob(JB):                        0.554\n",
      "Kurtosis:                       2.900   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Duplicate data\n",
    "x1_dup = np.concatenate((x1, x1), axis=0)\n",
    "y1_dup = np.concatenate((y1, y1), axis=0)\n",
    "\n",
    "model1_dup = OLS(endog=y1_dup, exog=x1_dup).fit()\n",
    "print(model1_dup.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.014\n",
      "Model:                            OLS   Adj. R-squared:                  0.007\n",
      "Method:                 Least Squares   F-statistic:                     2.045\n",
      "Date:                Sun, 27 Sep 2020   Prob (F-statistic):              0.131\n",
      "Time:                        21:42:37   Log-Likelihood:                -436.71\n",
      "No. Observations:                 300   AIC:                             877.4\n",
      "Df Residuals:                     298   BIC:                             884.8\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.1161      0.058      2.019      0.044       0.003       0.229\n",
      "x2             0.0013      0.061      0.021      0.983      -0.119       0.121\n",
      "==============================================================================\n",
      "Omnibus:                        2.596   Durbin-Watson:                   1.970\n",
      "Prob(Omnibus):                  0.273   Jarque-Bera (JB):                2.036\n",
      "Skew:                           0.035   Prob(JB):                        0.361\n",
      "Kurtosis:                       2.602   Cond. No.                         1.09\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Multicollinearity\n",
    "x2 = np.random.normal(loc=x1, scale=0.01, size=n)\n",
    "y2 = x1 - x2 + np.random.normal(size=n)\n",
    "x1x2 = np.resize(np.array((x1, x2)), (n, 2))\n",
    "\n",
    "model2 = OLS(endog=y2, exog=x1x2).fit()\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression (L1)\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|$$\n",
    "$$ \\hat{\\beta}_j^{lasso} = sgn(\\beta_j^{OLS}) max(0, |\\beta_j^{OLS}| - \\gamma), \\gamma = \\frac{n \\lambda}{||x||^2}$$\n",
    "\n",
    "Usage:\n",
    "- variable selection\n",
    "- parameter shrinkage\n",
    "- penalize $\\beta_j$ to exactly zero\n",
    "\n",
    "## Ridge Regression (L2)\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2$$\n",
    "$$ \\hat{\\beta}_j^{ridge} = (X^T X + \\lambda I_p)^{-1} X^T y$$\n",
    "\n",
    "Usage:\n",
    "- parameter shrinkage\n",
    "- include all independent variables\n",
    "- penalize $\\beta_j$ close to zero\n",
    "\n",
    "## Elastic Net (L1 + L2)\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 + \\alpha \\lambda \\sum_{j=1}^p \\beta_j^2 + (1 - \\alpha) \\lambda \\sum_{j=1}^p |\\beta_j|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Regularization Questions}$:\n",
    "- Need standardization before regularization\n",
    "  - Why: Transform features to same scale\n",
    "- Why does regularization reduce overfitting\n",
    "  - Large $\\lambda$ penalizes parameters/weights (close) to zero\n",
    "  - Reduce Model complexity\n",
    "- Why do not regularize bias term ($\\beta_0$)\n",
    "  - Var(X + $\\beta_0$) = Var(X)\n",
    "  - Bias term does not increase variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Each observation: \n",
    "$$f(y_i) = p^{y_i} (1-p)^{1-{y_i}}$$\n",
    "\n",
    "Likelihood:\n",
    "$$L(p) = \\prod_i f(y_i) = p^{\\sum_i y_i} (1-p)^{\\sum_i (1 - y_i)}$$\n",
    "\n",
    "Log-likelihood:\n",
    "$$\\mathcal{l} = log(L(p)) = \\sum_i y_i log(p) + \\sum_i (1 - y_i) log(1-p)$$\n",
    "\n",
    "Loss function (Cross Entropy):\n",
    "$$\\mathcal{L} = -\\sum_i [y_i log(p_i) + (1 - y_i) log(1 - p_i)]$$\n",
    "\n",
    "$\\textbf{Assumptions}$:\n",
    "- linearity of indepdent variables and log odds\n",
    "$$p = \\frac{1}{1 + exp(-z)}, z = ln \\left(\\frac{p}{1-p}\\right) = X \\beta$$\n",
    "- every observation is independent Bernoulli(p)\n",
    "- no multicollinearity\n",
    "\n",
    "$\\textbf{Derivations}$\n",
    "- Note: $$p_i (z) = \\sigma(z) = \\frac{1}{exp(-z) + 1}, \\sigma'(z) = \\sigma(z) (1 - \\sigma(z))$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = -\\sum_i \\left[y_i \\frac{p_i (1 - p_i)}{p_i} x_i + (1 - y_i) \\frac{-p_i (1 - p_i)}{(1 - p_i)} x_i\\right]$$\n",
    "$$= -\\sum_i \\left[y_i (1 - p_i) x_i + -(1 - y_i) p_i x_i \\right]$$\n",
    "$$= \\sum_i \\left[x_i (p_i - y_i) \\right] = X^T \\left( \\frac{1}{1 + exp(- X^T \\beta)} - y \\right)$$\n",
    "\n",
    "$\\textbf{Parameter Update}$:\n",
    "- Gradient Descent:\n",
    "  - $\\alpha$: learning rate\n",
    "  $$\\beta := \\beta - \\alpha * \\frac{\\partial \\mathcal{L}}{\\partial \\beta}$$\n",
    "- Quasi Newton's Method:\n",
    "  - $H(\\beta)$: Hessian Matrix\n",
    "  - $\\Delta_{\\beta}$: gradient of Loss w.r.t $\\beta$\n",
    "  $$\\beta := \\beta - H(\\beta)^{-1} * \\frac{\\partial \\mathcal{L}}{\\partial \\beta}$$\n",
    "  $$H(\\beta) = \\frac{\\partial \\Delta_{\\beta}}{\\partial \\beta} = X^T \\sigma(\\beta) (1-\\sigma(\\beta)) X, \\Delta_{\\beta} = \\frac{\\partial \\mathcal{L}}{\\partial \\beta}$$\n",
    "  \n",
    "$\\textbf{Train Logistic Regression on linear separable data}$:\n",
    "- larger $|\\beta|$, steeper the sigmoid curve \n",
    "- the vertical line $x=0$ separates two classes\n",
    "- $|\\beta|$ will approach $\\infty$\n",
    "- maximum likelihood likelihood does not exist ($\\infty$) regardless of $\\beta$\n",
    "\n",
    "$$\\lim_{\\beta\\to \\infty} \\sigma(\\beta, x) = 0$$\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/qoTul.png\" alt=\"logistic regression\" width=750>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 300\n",
    "dim = 1\n",
    "x1 = np.random.normal(size=(n, dim))\n",
    "z1 = 2 * x1 + np.random.normal(size=(n, 1)) + 3\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (np.exp(-x) + 1)\n",
    "\n",
    "y1 = sigmoid(z1) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression with gradient descent\n",
    "\n",
    "def log_gradient(X, beta, y):\n",
    "    return X.T.dot(sigmoid(X.dot(beta)) - y)\n",
    "\n",
    "def log_loss(X, beta, y):\n",
    "    p = sigmoid(X.dot(beta))\n",
    "    return -y.T.dot(p) - (1-y).T.dot(1-p)\n",
    "\n",
    "def gradient_descent(X, y, max_iter=1e3, tolerance=1e-4, learning_rate=1e-2, l2_regularization=1):\n",
    "    n_ob = X.shape[0]\n",
    "    n_iter = 0\n",
    "    \n",
    "    new_X = np.hstack((X,np.ones((n_ob,1))))\n",
    "    n_var = X.shape[-1]\n",
    "\n",
    "    beta = np.random.normal(size=(n_var+1, 1))\n",
    "    while n_iter < max_iter:\n",
    "        beta_gradient = log_gradient(new_X, beta, y) + l2_regularization * beta\n",
    "        beta -= learning_rate * beta_gradient\n",
    "        \n",
    "        # print(log_loss(new_X, beta, y))\n",
    "        \n",
    "        if all(x < tolerance for x in np.abs(learning_rate * beta_gradient)):\n",
    "            break\n",
    "        \n",
    "        n_iter += 1\n",
    "    \n",
    "    print(\"Number of iteration: \" + str(n_iter))\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteration: 144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.36273824],\n",
       "       [3.67469266]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(x1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_predict(X, y, optimizer=gradient_descent):\n",
    "    beta = optimizer(X, y)\n",
    "    prob = sigmoid(X.dot(beta[:-1,]) + beta[-1,])\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteration: 144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((log_predict(x1, y1) > 0.5) == y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression with quasi newton method\n",
    "\n",
    "def log_Hessian(X, beta, y):\n",
    "    p = sigmoid(X.dot(beta))\n",
    "    return X.T.dot(p * (1-p) * X)\n",
    "\n",
    "def quasi_newtown_method(X, y, max_iter=1e3, tolerance=1e-4, l2_regularization=1):\n",
    "    n_ob = X.shape[0]\n",
    "    n_iter = 0\n",
    "    \n",
    "    new_X = np.hstack((X,np.ones((n_ob,1))))\n",
    "    n_var = X.shape[-1]\n",
    "\n",
    "    beta = np.random.normal(size=(n_var+1, 1))\n",
    "    while n_iter < max_iter:\n",
    "        beta_gradient = log_gradient(new_X, beta, y) + l2_regularization * beta\n",
    "        beta_Hessian = log_Hessian(new_X, beta, y) + l2_regularization\n",
    "        \n",
    "        beta -= np.linalg.inv(beta_Hessian).dot(beta_gradient)\n",
    "        # print(log_loss(new_X, beta, y))\n",
    "        \n",
    "        if all(x < tolerance for x in np.abs(np.linalg.inv(beta_Hessian).dot(beta_gradient))):\n",
    "            break\n",
    "        \n",
    "        n_iter += 1\n",
    "    \n",
    "    print(\"Number of iteration: \" + str(n_iter))\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteration: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.36458159],\n",
       "       [3.67690198]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quasi_newtown_method(x1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteration: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((log_predict(x1, y1, quasi_newtown_method) > 0.5) == y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model1 = LogisticRegression(C=1)\n",
    "lr_model1.fit(x1, np.ravel(y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.951853622996335, 4.52335977167422)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model1.coef_.item(), lr_model1.intercept_.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model1.score(x1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression (Multinomial Logistic Regression)\n",
    "\n",
    "$$Pr(y_i = c | X_i) = \\frac{exp(X_i \\beta_c )}{\\sum_{k=1}^K exp(X_i \\beta_k)}$$\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{i=1}^{n} \\sum_{k=1}^K I(y_i=k) log \\left(p_i (y_i = c | X_i) \\right)$$\n",
    "\n",
    "$$z_i = X_i \\beta_c, \\frac{\\partial \\mathcal{L}}{\\partial z} = \\sum_{i=1}^n [p_i (y_i = c | X_i) - y_i] = \\overrightarrow{p} - \\overrightarrow{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Pseudo R-Squared}$: https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/#:~:text=A%20pseudo%20R%2Dsquared%20only,model%20better%20predicts%20the%20outcome.\n",
    "$$R^2 = 1 - \\frac{\\sum_i (y_i - p_i)^2}{\\sum_i (y_i - \\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Lasso Regression: https://stats.stackexchange.com/questions/17781/derivation-of-closed-form-lasso-solution\n",
    "- Logistic Regression in scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Linear Separable data in logistic regression: https://stats.stackexchange.com/questions/224863/understanding-complete-separation-for-logistic-regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
